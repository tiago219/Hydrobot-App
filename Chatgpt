
Aqui está o server_heuristic.py completo, já com o remapeamento de score aplicado (0.0–0.40 mapeado para 0.0–1.0).

Nada na detecção mudou, apenas a escala de saída:


# server_heuristic.py
# Detector contínuo com filtros e sensibilidade ajustados
# Agora com remapeamento do score final (0.0–0.40 → 0.0–1.0)

import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

JPEG_QUALITY = 85

# HSV mais amplo p/ fogo real
HSV_LOW = (8, 80, 120)
HSV_HIGH = (40, 255, 255)

# Dominância de vermelho (reforço)
RED_DELTA = 15

# Detector
DETECTOR_MAX_FPS = 14.0
HYST_HIGH = 0.62
HYST_LOW  = 0.48
VOTE_WINDOW = 7
VOTE_NEED   = 4
EMA_ALPHA   = 0.25
MIN_BLOB_AREA = 1200
KERNEL_SZ = 5

# Anti-movimento
MOTION_THRESH = 22
MOTION_DILATE_ITERS = 1

# Persistência espacial
PERSIST_CONSEC = 2
IOU_MIN = 0.15

# Idades máximas
MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== REMAP SCORE =====================
def remap_score(s: float, old_min=0.0, old_max=0.40, new_min=0.0, new_max=1.0) -> float:
    if s <= old_min:
        return new_min
    if s >= old_max:
        return new_max
    return (s - old_min) / (old_max - old_min) * (new_max - new_min) + new_min

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire (Balanced + Remap)", version="1.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP
        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5); continue
                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()
                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set(): break
                        if not chunk: continue
                        buf += chunk
                        if len(buf) > MAX_BYTES: buf = b""
                        i = buf.find(BOUNDARY)
                        if i == -1: continue
                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2
                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1: continue
                        headers_bytes = buf[hdr_start:headers_end]
                        content_length = None
                        for line in headers_bytes.split(b"\r\n"):
                            if line.lower().startswith(b"content-length:"):
                                try: content_length = int(line.split(b":", 1)[1].strip())
                                except: pass
                                break
                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length: continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue
                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None: return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms: return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== UTILs VISÃO =====================
def rgb_red_dominance_mask(frame_bgr: np.ndarray, delta: int = RED_DELTA) -> np.ndarray:
    b, g, r = cv2.split(frame_bgr)
    mask = (r.astype(np.int16) > (g.astype(np.int16) + delta)) & (r.astype(np.int16) > (b.astype(np.int16) + delta))
    return (mask.astype(np.uint8)) * 255

def hsv_fire_mask(frame_bgr: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
    lower = np.array(HSV_LOW, dtype=np.uint8)
    upper = np.array(HSV_HIGH, dtype=np.uint8)
    return cv2.inRange(hsv, lower, upper)

def skin_mask_ycrcb(frame_bgr: np.ndarray) -> np.ndarray:
    ycrcb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    skin = cv2.inRange(ycrcb, (0, 133, 77), (255, 173, 127))
    dark = cv2.threshold(y, 60, 255, cv2.THRESH_BINARY)[1]
    skin = cv2.bitwise_and(skin, dark)
    return skin

def iou(a: Tuple[int,int,int,int], b: Tuple[int,int,int,int]) -> float:
    ax, ay, aw, ah = a; bx, by, bw, bh = b
    ax2, ay2 = ax + aw, ay + ah; bx2, by2 = bx + bw, by + bh
    ix1, iy1 = max(ax, bx), max(ay, by)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)
    inter = iw * ih; union = aw*ah + bw*bh - inter
    return float(inter) / float(union) if union > 0 else 0.0

def boxes_from_mask(mask_bin: np.ndarray, min_area: int = MIN_BLOB_AREA) -> List[List[int]]:
    k = np.ones((KERNEL_SZ, KERNEL_SZ), np.uint8)
    m = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, k, 1)
    m = cv2.morphologyEx(m, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if w * h >= min_area:
            boxes.append([x, y, w, h])
    return boxes

# ===================== DETECTOR CONTÍNUO =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._persist_hits = 0
        self._last_main_box: Optional[Tuple[int,int,int,int]] = None

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            mask_hsv = hsv_fire_mask(frame)
            mask_skin = skin_mask_ycrcb(frame)
            mask_red  = rgb_red_dominance_mask(frame)

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            stable = cv2.bitwise_and(mask_hsv, cv2.bitwise_not(mask_skin))
            stable = cv2.bitwise_and(stable, cv2.bitwise_not(motion_mask))

            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            V = hsv[..., 2]
            bright = cv2.threshold(V, 200, 255, cv2.THRESH_BINARY)[1]
            red_boost = cv2.bitwise_and(mask_red, bright)
            combined = cv2.bitwise_or(stable, red_boost)

            ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
            v_mean = float(np.mean(V)) / 255.0
            score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)

            ratio_combined = float(np.count_nonzero(combined)) / float(combined.size)
            score_combined = min(1.0, ratio_combined * 5.0 + v_mean * 0.1)

            ema = score_combined if self._score_ema == 0.0 else (EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema)

            boxes = boxes_from_mask(combined, MIN_BLOB_AREA)

            main_box = None
            if boxes:
                areas = [w*h for (_,_,w,h) in boxes]
                main_box = boxes[int(np.argmax(areas))]
                if self._last_main_box is not None:
                    if iou(tuple(main_box), tuple(self._last_main_box)) >= IOU_MIN:
                        self._persist_hits += 1
                    else:
                        self._persist_hits = 1
                else:
                    self._persist_hits = 1
            else:
                self._persist_hits = 0
            self._last_main_box = tuple(main_box) if main_box is not None else None

            if ema >= HYST_HIGH and self._persist_hits >= PERSIST_CONSEC:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                guess = (1 if (len(self._votes) > 0 and self._votes[-1] == 1 and self._persist_hits >= PERSIST_CONSEC) else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = remap_score(float(ema))   # <<<<<< AQUI aplica o remap
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes if self._is_fire else []
                self._last_result_ts = int(time.time() * 1000)

            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
                "persist": {"hits": self._persist_hits, "need": PERSIST_CONSEC, "iou_min": IOU_MIN},
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
            }

detector = Detector(grabber)
detector.start()

def sync_detect_once(jpeg: bytes) -> Dict[str, Any]:
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    mask_hsv = hsv_fire_mask(frame)
    ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
    v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
    score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)
    return {
        "ok": True, "isFire": bool(score_raw >= HYST_HIGH),
        "score": round(remap_score(score_raw), 3),
        "score_raw": round(score_raw, 3),
        "score_ema": round(rem










































5

Entendi — vamos “afrouxar” sem voltar aos falsos positivos. Abaixo vai um único arquivo revisado, com ajustes bem equilibrados:




HSV mais amplo para pegar chamas reais (especialmente amareladas): HSV_LOW=(8,80,120).


Dominância de vermelho menos rígida (RED_DELTA=15) e usada como reforço, não bloqueio total.


Máscara combinada: (HSV ∧ ¬pele ∧ ¬movimento)  ⊕  reforço_por_vermelho_e_brilho

(o reforço ativa quando o pixel é bem claro e com vermelho dominante).


Persistência espacial suavizada: PERSIST_CONSEC=2, IOU_MIN=0.15.


Histerese/Votos menos conservadores: HYST_HIGH=0.62, HYST_LOW=0.48, VOTE_WINDOW=7, VOTE_NEED=4.


Área mínima reduzida para MIN_BLOB_AREA=1200.


Mantém detector contínuo e compat com o app (ok/isFire/score).




Substitua seu server_heuristic.py por este:


# server_heuristic.py
# Detector contínuo com filtros e sensibilidade ajustados para detectar fogo real
# sem voltar aos falsos positivos comuns (mãos rápidas etc).

import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

JPEG_QUALITY = 85

# HSV mais amplo p/ fogo real (amarelo/laranja)
HSV_LOW = (8, 80, 120)
HSV_HIGH = (40, 255, 255)

# Dominância de vermelho (reforço, não bloqueio)
RED_DELTA = 15  # R deve ser ao menos 15 maior que G e B para reforço

# Detector (equilíbrio sensibilidade/robustez)
DETECTOR_MAX_FPS = 14.0
HYST_HIGH = 0.62
HYST_LOW  = 0.48
VOTE_WINDOW = 7
VOTE_NEED   = 4
EMA_ALPHA   = 0.25
MIN_BLOB_AREA = 1200
KERNEL_SZ = 5

# Anti-movimento
MOTION_THRESH = 22
MOTION_DILATE_ITERS = 1

# Persistência espacial
PERSIST_CONSEC = 2
IOU_MIN = 0.15

# Idades máximas
MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire (Balanced)", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP
        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5); continue
                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()
                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set(): break
                        if not chunk: continue
                        buf += chunk
                        if len(buf) > MAX_BYTES: buf = b""
                        i = buf.find(BOUNDARY)
                        if i == -1: continue
                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2
                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1: continue
                        headers_bytes = buf[hdr_start:headers_end]
                        content_length = None
                        for line in headers_bytes.split(b"\r\n"):
                            if line.lower().startswith(b"content-length:"):
                                try: content_length = int(line.split(b":", 1)[1].strip())
                                except: pass
                                break
                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length: continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue
                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None: return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms: return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== UTILs VISÃO =====================
def rgb_red_dominance_mask(frame_bgr: np.ndarray, delta: int = RED_DELTA) -> np.ndarray:
    b, g, r = cv2.split(frame_bgr)
    mask = (r.astype(np.int16) > (g.astype(np.int16) + delta)) & (r.astype(np.int16) > (b.astype(np.int16) + delta))
    return (mask.astype(np.uint8)) * 255

def hsv_fire_mask(frame_bgr: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
    lower = np.array(HSV_LOW, dtype=np.uint8)
    upper = np.array(HSV_HIGH, dtype=np.uint8)
    return cv2.inRange(hsv, lower, upper)

def skin_mask_ycrcb(frame_bgr: np.ndarray) -> np.ndarray:
    ycrcb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    skin = cv2.inRange(ycrcb, (0, 133, 77), (255, 173, 127))
    dark = cv2.threshold(y, 60, 255, cv2.THRESH_BINARY)[1]
    skin = cv2.bitwise_and(skin, dark)
    return skin

def iou(a: Tuple[int,int,int,int], b: Tuple[int,int,int,int]) -> float:
    ax, ay, aw, ah = a; bx, by, bw, bh = b
    ax2, ay2 = ax + aw, ay + ah; bx2, by2 = bx + bw, by + bh
    ix1, iy1 = max(ax, bx), max(ay, by)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)
    inter = iw * ih; union = aw*ah + bw*bh - inter
    return float(inter) / float(union) if union > 0 else 0.0

def boxes_from_mask(mask_bin: np.ndarray, min_area: int = MIN_BLOB_AREA) -> List[List[int]]:
    k = np.ones((KERNEL_SZ, KERNEL_SZ), np.uint8)
    m = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, k, 1)
    m = cv2.morphologyEx(m, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if w * h >= min_area:
            boxes.append([x, y, w, h])
    return boxes

# ===================== DETECTOR CONTÍNUO =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._persist_hits = 0
        self._last_main_box: Optional[Tuple[int,int,int,int]] = None

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            # --- máscaras base ---
            mask_hsv = hsv_fire_mask(frame)
            mask_skin = skin_mask_ycrcb(frame)
            mask_red  = rgb_red_dominance_mask(frame)

            # anti-movimento
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            # combinação principal: HSV ∧ ¬pele ∧ ¬movimento
            stable = cv2.bitwise_and(mask_hsv, cv2.bitwise_not(mask_skin))
            stable = cv2.bitwise_and(stable, cv2.bitwise_not(motion_mask))

            # reforço: se pixel for muito claro e tiver dominância de vermelho, adiciona
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            V = hsv[..., 2]
            bright = cv2.threshold(V, 200, 255, cv2.THRESH_BINARY)[1]  # bem claro
            red_boost = cv2.bitwise_and(mask_red, bright)
            combined = cv2.bitwise_or(stable, red_boost)

            # scores
            ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
            v_mean = float(np.mean(V)) / 255.0
            score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)

            ratio_combined = float(np.count_nonzero(combined)) / float(combined.size)
            score_combined = min(1.0, ratio_combined * 5.0 + v_mean * 0.1)

            ema = score_combined if self._score_ema == 0.0 else (EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema)

            # caixas com base no combined (mais sensível que só stable)
            boxes = boxes_from_mask(combined, MIN_BLOB_AREA)

            # persistência espacial leve
            main_box = None
            if boxes:
                areas = [w*h for (_,_,w,h) in boxes]
                main_box = boxes[int(np.argmax(areas))]
                if self._last_main_box is not None:
                    if iou(tuple(main_box), tuple(self._last_main_box)) >= IOU_MIN:
                        self._persist_hits += 1
                    else:
                        self._persist_hits = 1
                else:
                    self._persist_hits = 1
            else:
                self._persist_hits = 0
            self._last_main_box = tuple(main_box) if main_box is not None else None

            # histerese + votos + persistência (agora mais permissivo)
            if ema >= HYST_HIGH and self._persist_hits >= PERSIST_CONSEC:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                guess = (1 if (len(self._votes) > 0 and self._votes[-1] == 1 and self._persist_hits >= PERSIST_CONSEC) else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes if self._is_fire else []
                self._last_result_ts = int(time.time() * 1000)

            # FPS detector
            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
                "persist": {"hits": self._persist_hits, "need": PERSIST_CONSEC, "iou_min": IOU_MIN},
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
            }

detector = Detector(grabber)
detector.start()

# Fallback síncrono se não houver resultado recente
def sync_detect_once(jpeg: bytes) -> Dict[str, Any]:
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    mask_hsv = hsv_fire_mask(frame)
    ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
    v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
    score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)
    is_fire = bool(score_raw >= HYST_HIGH)
    return {
        "ok": True, "isFire": is_fire,
        "score": round(float(score_raw), 3),
        "score_raw": round(float(score_raw), 3),
        "score_ema": round(float(score_raw), 3),
        "boxes": [], "ts": int(time.time() * 1000), "fallback": True
    }

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "balanced_detector",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    res = detector.get_result()
    if res.get("ts", 0) and (int(time.time() * 1000) - res["ts"] <= MAX_RESULT_AGE_MS):
        return res
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    return sync_detect_once(jpeg)



Se ainda ficar “tímido” para acender:




Baixe HYST_HIGH para 0.60.


Ou reduza VOTE_NEED para 3.


Ou aumente o reforço de brilho: troque bright para threshold(V, **185**, 255, ...).




Me diga como ficou com uma chama real (isqueiro/vela). Se precisar, coloco um endpoint /overlay com caixas desenhadas pra debug visual rápido.


Perfeito — vamos deixar a detecção bem menos sensível a mãos/objetos passando rápido. Abaixo está um server_heuristic.py completo que aplica várias salvaguardas anti-falso-positivo, mantendo compatibilidade com o app (ok/isFire/score):


O que mudou (resumo)




Histerese mais rígida: HYST_HIGH=0.70, HYST_LOW=0.50, com janela de votos maior (VOTE_WINDOW=8, precisa VOTE_NEED=6).


Anti-pele (skin): máscara em YCrCb remove tons de pele que frequentemente confundem com “fogo”.


Dominância de vermelho: exige R > G + Δ e R > B + Δ além do HSV laranja/amarelo (fogo real tem forte vermelho).


Anti-movimento aprimorado: ignora pixels que mudaram muito (mãos rápidas) e exige persistência espacial: o maior “blob de fogo” precisa persistir em posição por 3 quadros consecutivos (IoU > 0.25 com o anterior).


Área mínima maior e morfologia para filtrar ruídos.


EMA temporal continua suavizando o score.


Detector contínuo por frame (baixa latência), e /detect retorna o último resultado pronto (ou faz fallback síncrono se necessário).






Se ficar “duro demais”, te digo exatamente quais constantes afrouxar ao final.





server_heuristic.py (substitua o arquivo)


# server_heuristic.py
# FastAPI com:
# - Grabber contínuo do MJPEG (:81/stream)
# - Detector contínuo com filtros anti-falso-positivo:
#   * HSV laranja/amarelo + dominância de vermelho (RGB)
#   * Anti-pele (YCrCb)
#   * Anti-movimento + persistência espacial (IoU) por N quadros
#   * Histerese + janela de votos
# Endpoints compatíveis com o app: /snapshot e /detect

import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

JPEG_QUALITY = 85

# Fogo (HSV laranja/amarelo) — mais seletivo
HSV_LOW = (10, 120, 140)   # antes (5,80,120)
HSV_HIGH = (35, 255, 255)

# Dominância de vermelho (RGB)
RED_DELTA = 25             # R precisa superar G e B por pelo menos 25

# Detector (menos sensível a ruído)
DETECTOR_MAX_FPS = 14.0
HYST_HIGH = 0.70
HYST_LOW  = 0.50
VOTE_WINDOW = 8
VOTE_NEED   = 6
EMA_ALPHA   = 0.25
MIN_BLOB_AREA = 1800       # maior área mínima
KERNEL_SZ = 5

# Anti-movimento
MOTION_THRESH = 22
MOTION_DILATE_ITERS = 1

# Persistência espacial (IoU)
PERSIST_CONSEC = 3         # precisa de 3 frames seguidos batendo posição
IOU_MIN = 0.25

# Idades máximas
MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire (Robusto anti-false positive)", version="0.9.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP
        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5)
                        continue

                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()

                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set():
                            break
                        if not chunk:
                            continue
                        buf += chunk
                        if len(buf) > MAX_BYTES:
                            buf = b""

                        i = buf.find(BOUNDARY)
                        if i == -1:
                            continue

                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2

                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1:
                            continue

                        headers_bytes = buf[hdr_start:headers_end]
                        # content-length opcional
                        content_length = None
                        for line in headers_bytes.split(b"\r\n"):
                            if line.lower().startswith(b"content-length:"):
                                try:
                                    content_length = int(line.split(b":", 1)[1].strip())
                                except:
                                    pass
                                break

                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length:
                                continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue

                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms

                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None:
                return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms:
                return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== UTILs VISÃO =====================
def rgb_red_dominance_mask(frame_bgr: np.ndarray, delta: int = RED_DELTA) -> np.ndarray:
    b, g, r = cv2.split(frame_bgr)
    # r > g + delta e r > b + delta
    mask = (r.astype(np.int16) > (g.astype(np.int16) + delta)) & (r.astype(np.int16) > (b.astype(np.int16) + delta))
    return (mask.astype(np.uint8)) * 255

def hsv_fire_mask(frame_bgr: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
    lower = np.array(HSV_LOW, dtype=np.uint8)
    upper = np.array(HSV_HIGH, dtype=np.uint8)
    return cv2.inRange(hsv, lower, upper)

def skin_mask_ycrcb(frame_bgr: np.ndarray) -> np.ndarray:
    # máscara de pele típica em YCrCb
    ycrcb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    skin = cv2.inRange(ycrcb, (0, 133, 77), (255, 173, 127))  # faixa clássica
    # reforço: regiões muito escuras não contam
    dark = cv2.threshold(y, 60, 255, cv2.THRESH_BINARY)[1]
    skin = cv2.bitwise_and(skin, dark)
    return skin  # 255 = pele

def iou(a: Tuple[int,int,int,int], b: Tuple[int,int,int,int]) -> float:
    ax, ay, aw, ah = a
    bx, by, bw, bh = b
    a1, a2 = ax, ay
    a3, a4 = ax + aw, ay + ah
    b1, b2 = bx, by
    b3, b4 = bx + bw, by + bh
    ix1, iy1 = max(a1, b1), max(a2, b2)
    ix2, iy2 = min(a3, b3), min(a4, b4)
    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)
    inter = iw * ih
    union = aw * ah + bw * bh - inter
    return float(inter) / float(union) if union > 0 else 0.0

def boxes_from_mask(mask_bin: np.ndarray, min_area: int = MIN_BLOB_AREA) -> List[List[int]]:
    k = np.ones((KERNEL_SZ, KERNEL_SZ), np.uint8)
    m = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, k, 1)
    m = cv2.morphologyEx(m, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if w * h >= min_area:
            boxes.append([x, y, w, h])
    return boxes

# ===================== DETECTOR CONTÍNUO =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._persist_hits = 0
        self._last_main_box: Optional[Tuple[int,int,int,int]] = None

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            # --- máscaras básicas ---
            mask_hsv = hsv_fire_mask(frame)
            mask_red = rgb_red_dominance_mask(frame)
            mask_skin = skin_mask_ycrcb(frame)

            # remove pele e exige dominância de vermelho
            mask_fire = cv2.bitwise_and(mask_hsv, mask_red)
            mask_fire = cv2.bitwise_and(mask_fire, cv2.bitwise_not(mask_skin))

            # --- anti-movimento ---
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            # Só consideramos pixels de fogo que NÃO são movimento abrupto
            stable_mask = cv2.bitwise_and(mask_fire, cv2.bitwise_not(motion_mask))

            # Score “bruto” pelo HSV (proxy de quão “fogo” está)
            ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
            v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
            score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)

            # Score “estável” dá mais peso ao que não é movimento
            ratio_stable = float(np.count_nonzero(stable_mask)) / float(stable_mask.size)
            score_stable = min(1.0, ratio_stable * 6.0 + v_mean * 0.1)

            # Combinação (mais conservadora)
            score_combined = 0.35 * score_raw + 0.65 * score_stable

            # EMA temporal
            ema = score_combined if self._score_ema == 0.0 else (EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema)

            # caixas de candidatos (a partir do estável)
            boxes = boxes_from_mask(stable_mask, MIN_BLOB_AREA)

            # persistência espacial: caixa principal precisa “manter posição” por alguns frames
            main_box = None
            if boxes:
                # escolhe maior área
                areas = [w*h for (_,_,w,h) in boxes]
                main_box = boxes[int(np.argmax(areas))]

                if self._last_main_box is not None and main_box is not None:
                    i = iou(tuple(main_box), tuple(self._last_main_box))
                    if i >= IOU_MIN:
                        self._persist_hits += 1
                    else:
                        self._persist_hits = 1  # reseta sequência
                else:
                    self._persist_hits = 1
            else:
                self._persist_hits = 0

            self._last_main_box = tuple(main_box) if main_box is not None else None

            # histerese + votos + persistência espacial
            if ema >= HYST_HIGH and self._persist_hits >= PERSIST_CONSEC:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                # zona morta: apoia-se no histórico, mas exige persistência mínima
                guess = 1 if (len(self._votes) > 0 and self._votes[-1] == 1 and self._persist_hits >= PERSIST_CONSEC) else 0

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            # atualiza estado
            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes if self._is_fire else []
                self._last_result_ts = int(time.time() * 1000)

            # FPS detector
            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            # respeita teto FPS
            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),   # compat com app
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
                "persist": {"hits": self._persist_hits, "need": PERSIST_CONSEC, "iou_min": IOU_MIN},
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
            }

detector = Detector(grabber)
detector.start()

# Fallback síncrono: caso ainda não haja resultado recente
def sync_detect_once(jpeg: bytes) -> Dict[str, Any]:
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}

    mask_hsv = hsv_fire_mask(frame)
    mask_red = rgb_red_dominance_mask(frame)
    mask_skin = skin_mask_ycrcb(frame)

    mask_fire = cv2.bitwise_and(mask_hsv, mask_red)
    mask_fire = cv2.bitwise_and(mask_fire, cv2.bitwise_not(mask_skin))

    ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
    v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
    score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)
    is_fire = bool(score_raw >= HYST_HIGH)  # conservador no fallback

    return {
        "ok": True, "isFire": is_fire,
        "score": round(float(score_raw), 3),
        "score_raw": round(float(score_raw), 3),
        "score_ema": round(float(score_raw), 3),
        "boxes": [], "ts": int(time.time() * 1000), "fallback": True
    }

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "robust_detector",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)  # reinicia stream
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    res = detector.get_result()
    if res.get("ts", 0) and (int(time.time() * 1000) - res["ts"] <= MAX_RESULT_AGE_MS):
        return res
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    return sync_detect_once(jpeg)




Parametrização rápida (caso precise calibrar)






Quer ainda menos sensível?




Aumente HYST_HIGH para 0.75.


Aumente VOTE_NEED para 7.


Aumente PERSIST_CONSEC para 4.


Aumente MIN_BLOB_AREA para 2400.








Quer um pouco mais sensível (mas ainda robusto)?




Diminua HYST_HIGH para 0.65.


VOTE_NEED = 5.


PERSIST_CONSEC = 2.










Dica: abra http://IP_DO_PC:8000/status no navegador para ver score, isFire, vote.sum, persist.hits e ir ajustando “ao vivo”.




Você não precisa mudar nada no app. Assim que trocar este server_heuristic.py e reiniciar o uvicorn, os falsos positivos por mão/objetos rápidos devem cair bastante. Se ainda sobrar algum cenário chato, me descreva a iluminação e o que aparece na imagem; eu ajusto as faixas HSV/skin e os deltas de vermelho em cima disso.










































Valeu pelo retorno — vamos deixar detectando de novo agora. O problema foi o servidor novo ter ficado conservador demais (anti-movimento + votos) e também porque o JSON já não tinha o campo score que o app espera. Abaixo te mando um server_heuristic.py revisado que:




Mantém o grabber contínuo (sem reabrir o stream).


Detecta em outra thread (quase por frame), mais permissivo (vai voltar a acusar quando realmente houver fogo).


Compatível com o app: volta a devolver score (além de score_raw/score_ema).


Fallback: se por algum motivo ainda não houver resultado recente, o /detect processa sincronamente o frame atual e responde na hora.


Parâmetros de filtro menos agressivos (menos bloqueio por movimento / votos).




Substitui seu arquivo atual por este:


# server_heuristic.py
# FastAPI com:
# - Grabber contínuo do MJPEG (:81/stream)
# - Detector contínuo (~por frame) mais permissivo
# - Compat com o app: devolve 'score', 'isFire', 'ok'
# - Fallback síncrono no /detect se ainda não tiver resultado recente

import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG PRINCIPAL =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"      # boundary do ESP32-CAM
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

# Snapshot/encode
JPEG_QUALITY = 85

# Heurística cor de fogo (HSV laranja/amarelo)
HSV_LOW = (5, 80, 120)
HSV_HIGH = (40, 255, 255)

# Detector (frequência e filtros) — MAIS PERMISSIVO
DETECTOR_MAX_FPS = 14.0       # tenta processar até ~14 fps
HYST_HIGH = 0.60              # liga
HYST_LOW  = 0.40              # desliga
VOTE_WINDOW = 6               # janela de votos
VOTE_NEED   = 3               # precisa de 3 positivos na janela de 6
EMA_ALPHA   = 0.25            # suavização temporal
MIN_BLOB_AREA = 900           # área mínima para caixa

# Anti-movimento (relaxado)
MOTION_THRESH = 22            # threshold |frame - prev|
MOTION_DILATE_ITERS = 1

# Idade máxima aceitável para frame/resultado (ms)
MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire (Per-frame Detector, relaxed)", version="0.7.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP

        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5)
                        continue

                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()

                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set():
                            break
                        if not chunk:
                            continue
                        buf += chunk
                        if len(buf) > MAX_BYTES:
                            buf = b""

                        i = buf.find(BOUNDARY)
                        if i == -1:
                            continue

                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2

                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1:
                            continue

                        headers_bytes = buf[hdr_start:headers_end]
                        headers_text = headers_bytes.decode("latin1", errors="ignore")

                        content_length = None
                        for line in headers_text.split("\r\n"):
                            if line.lower().startswith("content-length:"):
                                try:
                                    content_length = int(line.split(":", 1)[1].strip())
                                except:
                                    pass
                                break

                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length:
                                continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue

                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms

                            # FPS de entrada do grabber
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None:
                return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms:
                return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== DETECTOR CONTÍNUO =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _color_score(self, frame_bgr: np.ndarray) -> Tuple[float, np.ndarray]:
        hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, HSV_LOW, HSV_HIGH)
        ratio = float(np.count_nonzero(mask)) / float(mask.size)
        v_mean = float(np.mean(hsv[..., 2])) / 255.0
        score = min(1.0, ratio * 4.0 + v_mean * 0.2)
        return score, mask

    def _boxes_from_mask(self, frame_bgr: np.ndarray, mask: np.ndarray) -> List[List[int]]:
        h, w = frame_bgr.shape[:2]
        scl = min(640, w)
        rs = cv2.resize(mask, (scl, int(h * scl / w)), interpolation=cv2.INTER_NEAREST)
        k = np.ones((5, 5), np.uint8)
        rs = cv2.morphologyEx(rs, cv2.MORPH_OPEN, k, 1)
        rs = cv2.morphologyEx(rs, cv2.MORPH_DILATE, k, 1)
        cnts, _ = cv2.findContours(rs, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        rx = w / float(scl)
        ry = h / float(rs.shape[0])
        boxes: List[List[int]] = []
        for c in cnts:
            x, y, bw, bh = cv2.boundingRect(c)
            if bw * bh < MIN_BLOB_AREA:
                continue
            boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
        return boxes

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01)
                continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005)
                continue

            # ----- cor de fogo -----
            score_color, fire_mask = self._color_score(frame)

            # ----- movimento (relaxado) -----
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            # estável = cor & ~movimento (ainda usamos, mas só para parte do score)
            stable_mask = cv2.bitwise_and(fire_mask, cv2.bitwise_not(motion_mask))
            ratio_stable = float(np.count_nonzero(stable_mask)) / float(stable_mask.size)

            # score combinado (70% cor + 30% estável) — mais permissivo
            score_combined = min(1.0, 0.7 * score_color + 0.3 * (ratio_stable * 5.0))

            # EMA temporal
            if self._score_ema == 0.0:
                ema = score_combined
            else:
                ema = EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema

            # histerese + votos (relaxado)
            if ema >= HYST_HIGH:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                guess = (self._votes[-1] if len(self._votes) > 0 else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            # caixas a partir da máscara de cor (para não perder alvos verdadeiros)
            boxes = self._boxes_from_mask(frame, fire_mask) if final_fire else []

            with self._lock:
                self._score_raw = float(score_color)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes
                self._last_result_ts = int(time.time() * 1000)

            # FPS do detector
            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            # teto de FPS
            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            # compat com o app: 'score' presente (usa o EMA)
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
            }

detector = Detector(grabber)
detector.start()

# ===== util síncrono para fallback no /detect =====
def sync_detect_once(jpeg: bytes) -> Dict[str, Any]:
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    fire_mask = cv2.inRange(hsv, HSV_LOW, HSV_HIGH)
    ratio = float(np.count_nonzero(fire_mask)) / float(fire_mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    score_color = min(1.0, ratio * 4.0 + v_mean * 0.2)

    # score direto (sem EMA/histerese) — só para fallback
    is_fire = bool(score_color >= HYST_HIGH)

    return {
        "ok": True, "isFire": is_fire,
        "score": round(float(score_color), 3),
        "score_raw": round(float(score_color), 3),
        "score_ema": round(float(score_color), 3),
        "boxes": [], "ts": int(time.time() * 1000), "fallback": True
    }

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "per_frame_detector_relaxed",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)  # reinicia stream
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    # usa resultado contínuo se recente; senão, processa síncrono UMA vez
    res = detector.get_result()
    if res.get("ts", 0) and (int(time.time() * 1000) - res["ts"] <= MAX_RESULT_AGE_MS):
        return res

    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    return sync_detect_once(jpeg)



O que você precisa no app?


Nada — ele já consome ok/isFire/score.

Se quiser checar se está rodando: abra no navegador do celular http://IP_DO_PC:8000/status e verifique:




hasFrame: true


fps_in > 0


campos score e isFire mudando quando há chama.




Se ainda ficar muito “mole” ou “duro”, ajustes rápidos:




Mais sensível (detectar mais): diminua HYST_HIGH para 0.55 e/ou VOTE_NEED=2.


Mais conservador (menos falso-positivo): aumente HYST_HIGH para 0.65 e/ou VOTE_NEED=4.




Me diga como ficou — se preferir, coloco um endpoint /overlay que desenha as caixas no JPEG para você visualizar no browser.














































Excelente ponto. Dá pra deixar a detecção “quase por frame” no servidor e ainda reduzir falso-positivos com filtros temporais + anti-movimento + histerese.

Abaixo te mando:






server_heuristic.py completo (com grabber contínuo + detector contínuo em outra thread):




Lê o MJPEG sem parar (já tínhamos).


Processa cada novo frame (até DETECTOR_MAX_FPS) — sem reabrir stream.


Histerese (dois thresholds alto/baixo).


Janela de votos (persistência) para confirmar/limpar fogo.


Anti-movimento: ignora pixels que são só “movimento brusco” (máscara de diferença de quadros).


Caixa(s) desenháveis via fire_boxes em cima do mask estável (cor ∧ não-movimento).


/detect passa a devolver o último resultado já pronto (latência baixíssima).








Pequeno ajuste opcional no app: se quiser, reduza o polling do /detect para ~100–120 ms (não é obrigatório, pois o servidor já detecta sozinho por frame).







1) server_heuristic.py (COMPLETO – detector por frame + filtros anti-ruído)


# server_heuristic.py
# FastAPI com:
# - Grabber contínuo do MJPEG (:81/stream) mantendo o último JPEG em RAM
# - Detector contínuo (thread separada) processando quase todos os frames
#   com histerese, janela de votos (persistência) e máscara anti-movimento
# Endpoints /snapshot e /detect respondem instantaneamente sem reabrir o stream.

import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG PRINCIPAL =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"      # boundary do ESP32-CAM
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

# Snapshot/encode
JPEG_QUALITY = 85

# Heurística de fogo (HSV para laranja/amarelo)
HSV_LOW = (5, 80, 120)
HSV_HIGH = (40, 255, 255)

# Detector (frequência e filtros)
DETECTOR_MAX_FPS = 12.0         # processa até ~12 fps (ajuste conforme seu PC)
HYST_HIGH = 0.65                # limiar para "ligar" fogo
HYST_LOW  = 0.45                # limiar para "desligar" fogo
VOTE_WINDOW = 8                 # janela de votos (frames)
VOTE_NEED   = 5                 # nº de votos "fogo" para confirmar
EMA_ALPHA   = 0.25              # peso do score novo no EMA
MIN_BLOB_AREA = 1200            # área mínima (px) para aceitar contorno de fogo

# Anti-movimento
MOTION_THRESH = 18              # threshold do |frame - prev| (em escala 0..255)
MOTION_DILATE_ITERS = 1

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire (Per-frame Detector)", version="0.6.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP

        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5)
                        continue

                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()

                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set():
                            break
                        if not chunk:
                            continue
                        buf += chunk
                        if len(buf) > MAX_BYTES:
                            buf = b""

                        i = buf.find(BOUNDARY)
                        if i == -1:
                            continue

                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2

                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1:
                            continue

                        headers_bytes = buf[hdr_start:headers_end]
                        headers_text = headers_bytes.decode("latin1", errors="ignore")

                        content_length = None
                        for line in headers_text.split("\r\n"):
                            if line.lower().startswith("content-length:"):
                                try:
                                    content_length = int(line.split(":", 1)[1].strip())
                                except:
                                    pass
                                break

                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length:
                                continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue

                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms

                            # FPS de entrada do grabber
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = 3000) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None:
                return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms:
                return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== DETECTOR CONTÍNUO =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        # estado do detector
        self._last_proc_ts: int = 0
        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _fire_score(self, frame_bgr: np.ndarray) -> Tuple[float, np.ndarray]:
        hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, HSV_LOW, HSV_HIGH)
        ratio = float(np.count_nonzero(mask)) / float(mask.size)
        v_mean = float(np.mean(hsv[..., 2])) / 255.0
        score = min(1.0, ratio * 4.0 + v_mean * 0.2)
        return score, mask

    def _stable_boxes(self, frame_bgr: np.ndarray, stable_mask: np.ndarray) -> List[List[int]]:
        h, w = frame_bgr.shape[:2]
        scl = min(640, w)
        rs = cv2.resize(stable_mask, (scl, int(h * scl / w)), interpolation=cv2.INTER_NEAREST)
        k = np.ones((5, 5), np.uint8)
        rs = cv2.morphologyEx(rs, cv2.MORPH_OPEN, k, 1)
        rs = cv2.morphologyEx(rs, cv2.MORPH_DILATE, k, 1)
        cnts, _ = cv2.findContours(rs, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        rx = w / float(scl)
        ry = h / float(rs.shape[0])
        boxes: List[List[int]] = []
        for c in cnts:
            x, y, bw, bh = cv2.boundingRect(c)
            if bw * bh < MIN_BLOB_AREA:
                continue
            boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
        return boxes

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg(max_age_ms=3000)
            if jpeg is None:
                time.sleep(0.02)
                continue

            # evite reprocessar caso venha jpeg repetido da mesma ts
            # (como usamos get_latest, a ts é aproximadamente monotônica no grabber)
            now_ms = int(time.time() * 1000)
            if now_ms - self._last_result_ts < int(min_interval * 1000 * 0.8):
                # seguro: respeita o teto de FPS configurado
                time.sleep(0.005)
                continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.01)
                continue

            # --- fire mask + anti-movimento ---
            score_raw, fire_mask = self._fire_score(frame)

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)

            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)

            self._prev_gray = gray

            # estável = cor de fogo E NÃO movimento
            stable_mask = cv2.bitwise_and(fire_mask, cv2.bitwise_not(motion_mask))

            # recompute score em cima do "estável"
            ratio_stable = float(np.count_nonzero(stable_mask)) / float(stable_mask.size)
            v_mean = float(np.mean(gray)) / 255.0
            score_stable = min(1.0, ratio_stable * 5.0 + v_mean * 0.1)   # leve reforço no peso da área estável

            # EMA temporal
            if self._score_ema == 0.0:
                ema = score_stable
            else:
                ema = EMA_ALPHA * score_stable + (1.0 - EMA_ALPHA) * self._score_ema

            # Histerese + Votos
            # 1) sugestão binária com histerese
            if ema >= HYST_HIGH:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                # zona morta: mantém última tendência (considera último voto se houver)
                guess = (self._votes[-1] if len(self._votes) > 0 else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            # Caixas só quando final_fire==1, com base no stable_mask
            boxes: List[List[int]] = self._stable_boxes(frame, stable_mask) if final_fire else []

            # Atualiza estado compartilhado
            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes
                self._last_result_ts = int(time.time() * 1000)

            # FPS do detector
            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            # respeita o teto de FPS
            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "buf": list(self._votes)},
            }

detector = Detector(grabber)
detector.start()

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "per_frame_detector",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)  # reinicia stream
    # detector não precisa reiniciar; ele consome do grabber
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=3000)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    # retorna o último resultado MÁXIMO-rápido (já processado na thread)
    return detector.get_result()



Como isso reduz falso-positivos:




Histerese (HYST_HIGH/LOW) evita “liga/desliga” por ruído.


Janela de votos (VOTE_WINDOW/VOTE_NEED) exige persistência por múltiplos frames.


Anti-movimento: ignora regiões que mudaram demais entre frames (normalmente reflexos ou variações rápidas).


EMA suaviza o score ao longo do tempo.




Se ainda quiser mais conservador, aumente VOTE_NEED para 6–7 e/ou suba HYST_HIGH para ~0.7.



2) (Opcional) App – diminuir o intervalo do /detect


No seu CameraScreen.tsx, na loop de detecção, troque a linha do setTimeout (estava 500ms / 200ms) para algo entre 100 e 150 ms:


// finalmente do loop de detecção
if (!stop) setTimeout(loop, 120); // ~8 Hz (o servidor já detecta por frame)





Como o servidor já está detectando continuamente, o app só consulta o último resultado — isso deixa a UI responsiva sem sobrecarregar nada.





Se quiser, posso te mandar só o trecho do CameraScreen.tsx com esse número alterado. Quer também um endpoint /overlay que já devolve o JPEG com caixas desenhadas (útil para debug visual)?







































Perfeito! Aqui estão os dois arquivos completos com as alterações para reduzir o “picote” e deixar a detecção mais frequente:



1) server_heuristic.py (grabber contínuo do MJPEG + snapshot instantâneo)


# server_heuristic.py
# FastAPI com "grabber" contínuo do MJPEG do ESP32-CAM (porta 81/stream).
# Lê o stream em uma thread, guarda o último JPEG e serve instantaneamente em /snapshot e /detect.

import time
import threading
from typing import Optional, List
import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ---------------- CONFIG GERAL ----------------
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"    # boundary do ESP
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0
JPEG_QUALITY = 85
SCORE_THRESHOLD = 0.60

# ---------------- FASTAPI ----------------
app = FastAPI(title="HydroBot Fire (Grabber contínuo)", version="0.4.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ---------------- PLACEHOLDER ----------------
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ---------------- GRABBER CONTÍNUO ----------------
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP

        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        # reconecta automaticamente em caso de erro
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.6)
                        continue

                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()

                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set():
                            break
                        if not chunk:
                            continue
                        buf += chunk
                        if len(buf) > MAX_BYTES:
                            buf = b""  # evita crescimento infinito

                        # encontre boundary
                        i = buf.find(BOUNDARY)
                        if i == -1:
                            continue

                        # pule CRLF após boundary
                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2

                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1:
                            continue

                        headers_bytes = buf[hdr_start:headers_end]
                        headers_text = headers_bytes.decode("latin1", errors="ignore")

                        content_length = None
                        for line in headers_text.split("\r\n"):
                            if line.lower().startswith("content-length:"):
                                try:
                                    content_length = int(line.split(":", 1)[1].strip())
                                except:
                                    pass
                                break

                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length:
                                continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue

                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms
                            # FPS
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.6)
            except Exception:
                time.sleep(0.6)

    def get_latest_jpeg(self, max_age_ms: int = 3000) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None:
                return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms:
                return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {
                "ip": self._ip,
                "hasFrame": self._last_jpeg is not None,
                "age_ms": age_ms,
                "fps_in": round(self._fps, 2),
                "ts_ms": self._last_ts_ms,
            }

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ---------------- FIRE HEURISTIC ----------------
def decode_jpeg_to_bgr(jpeg_bytes: bytes):
    arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

def fire_score(frame) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
            continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

# ---------------- ENDPOINTS ----------------
@app.get("/status")
def status():
    s = grabber.status()
    return {"ok": True, "model": "grabber_continuo", "threshold": SCORE_THRESHOLD, **s}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)  # reinicia o grabber no novo IP
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=5000)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    jpeg = grabber.get_latest_jpeg(max_age_ms=5000)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    frame = decode_jpeg_to_bgr(jpeg)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    s = fire_score(frame)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "threshold": SCORE_THRESHOLD,
        "boxes": fire_boxes(frame) if is_fire else [],
        "ts": int(time.time() * 1000),
    }




2) app/screens/CameraScreen.tsx (double-buffer com trava + polling mais rápido)




Snapshots agora em ~6–7 fps (FPS_INTERVAL = 150).


Detecção em ~5 Hz (200 ms).


Mantém crossfade sem flicker.




// app/screens/CameraScreen.tsx
import { useEffect, useMemo, useRef, useState } from "react";
import {
  View, Text, TextInput, Pressable, StyleSheet, PanResponder,
  GestureResponderEvent, PanResponderGestureState, Image, Animated,
} from "react-native";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

// -------- Crossfade com trava: só muda quando o próximo terminar de carregar --------
function CrossfadeImage({
  currentUri,
  nextUri,
  onNextLoadStart,
  onNextShown,
}: {
  currentUri: string;
  nextUri: string;
  onNextLoadStart: () => void;
  onNextShown: () => void;
}) {
  const fade = useRef(new Animated.Value(0)).current;
  const [showNext, setShowNext] = useState(false);
  const lastNextRef = useRef<string>("");

  useEffect(() => {
    if (!nextUri || nextUri === lastNextRef.current) return;
    lastNextRef.current = nextUri;
    setShowNext(true);
  }, [nextUri]);

  return (
    <View style={{ flex: 1, backgroundColor: "black" }}>
      {currentUri ? (
        <Image source={{ uri: currentUri }} style={{ flex: 1, width: "100%" }} resizeMode="contain" />
      ) : null}

      {showNext ? (
        <Animated.View style={[StyleSheet.absoluteFill, { opacity: fade }]}>
          <Image
            source={{ uri: nextUri }}
            style={{ flex: 1, width: "100%" }}
            resizeMode="contain"
            onLoadStart={onNextLoadStart}
            onError={() => {
              // fallback: promove mesmo assim; servidor tende a devolver placeholder JPEG
              onNextShown();
            }}
            onLoadEnd={() => {
              Animated.timing(fade, { toValue: 1, duration: 120, useNativeDriver: true }).start(() => {
                onNextShown();
                fade.setValue(0);
                setShowNext(false);
              });
            }}
          />
        </Animated.View>
      ) : null}
    </View>
  );
}

const textsByLang = {
  pt: {
    espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Salvando...",
    statusFail: "Falha ao conectar. Confira o Wi-Fi e IP.",
    noVideo: "Sem vídeo (snapshot). Verifique o servidor.",
    fireOn: (s:number)=>`🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sem fogo • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento salvo em ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  en: {
    espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:",
    detect: "Detect", detecting: "Detecting", saving: "Saving...",
    statusFail: "Failed to connect. Check Wi-Fi/IP.",
    noVideo: "No video (snapshot). Check the server.",
    fireOn: (s:number)=>`🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s:number)=>`No fire • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Event saved at ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  es: {
    espIp: "ESP IP:", test: "Probar", ledOn: "LED ENC.", ledOff: "LED APAG.",
    pumpOn: "BOMBA ENC.", pumpOff: "BOMBA APAG.", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Guardando...",
    statusFail: "Error de conexión. Revisa Wi-Fi/IP.",
    noVideo: "Sin vídeo (snapshot). Revisa el servidor.",
    fireOn: (s:number)=>`🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sin fuego • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento guardado a las ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
};

const DEFAULT_IP = "192.168.4.1";
const DEFAULT_SERVER = "http://192.168.4.2:8000";

export default function CameraScreen() {
  const { lang } = useLanguage();
  const T = textsByLang[lang];

  // conexões
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState("—");
  const [ledOn, setLedOn] = useState(false);
  const [pumpOn, setPumpOn] = useState(false);

  // detecção
  const [detectOn, setDetectOn] = useState(false);
  const [isFire, setIsFire] = useState(false);
  const [fireScore, setFireScore] = useState(0);

  // registro automático
  const [saving, setSaving] = useState(false);
  const lastSaveRef = useRef(0);
  const wasFireRef = useRef(false);
  const SAVE_COOLDOWN_MS = 5000;

  // vídeo (double buffer + trava)
  const [currentFrameUri, setCurrentFrameUri] = useState("");
  const [nextFrameUri, setNextFrameUri] = useState("");
  const loadingNextRef = useRef(false);

  // joystick (UI)
  const [joy, setJoy] = useState({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs do ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  // ===== ESP STATUS =====
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode} • led:${j.led ? "on" : "off"} • pump:${j.pump ? "on" : "off"}`);
    } catch {
      setStatusText(T.statusFail);
    } finally {
      setIsChecking(false);
    }
  }
  async function toggleLed() { try { const t = !ledOn; await fetch(ledUrl(t)); setLedOn(t); } catch { setStatusText(T.statusFail); } }
  async function togglePump() { try { const t = !pumpOn; await fetch(pumpUrl(t)); setPumpOn(t); } catch { setStatusText(T.statusFail); } }

  useEffect(() => { pingStatus(); const id = setInterval(pingStatus, 5000); return () => clearInterval(id); }, [statusUrl]);

  // ===== Vídeo via snapshots — mais rápido e SEM flicker =====
  useEffect(() => {
    let stop = false;
    const FPS_INTERVAL = 150; // ~6–7 fps

    // primeiro frame (placeholder/real) para não ficar tela preta
    setCurrentFrameUri(`${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`);

    const tick = () => {
      if (stop) return;
      if (!loadingNextRef.current) {
        const url = `${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`;
        setNextFrameUri(url);
      }
      setTimeout(tick, FPS_INTERVAL);
    };
    tick();

    return () => { stop = true; };
  }, [server]);

  function onNextLoadStart() {
    loadingNextRef.current = true;
  }
  function onNextShown() {
    if (nextFrameUri) setCurrentFrameUri(nextFrameUri);
    loadingNextRef.current = false;
  }

  // ===== DETECT =====
  useEffect(() => {
    if (!detectOn) return;
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server.replace(/\/+$/, "")}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && j.ok) {
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));
        }
      } catch {
        if (!stop) { setIsFire(false); setFireScore(0); }
      } finally {
        if (!stop) setTimeout(loop, 200); // ~5 Hz
      }
    };
    loop();
    return () => { stop = true; };
  }, [detectOn, server]);

  // ===== Registro automático =====
  useEffect(() => {
    const rising = !wasFireRef.current && isFire;
    const now = Date.now();
    const cooldownOk = now - lastSaveRef.current >= SAVE_COOLDOWN_MS;

    if (detectOn && rising && cooldownOk && !saving) {
      (async () => {
        try {
          setSaving(true);
          await saveEventFromServer(server);
          lastSaveRef.current = Date.now();
          setStatusText(T.eventSavedAt(new Date(lastSaveRef.current).toLocaleString()));
        } catch {
          setStatusText(T.noVideo);
        } finally {
          setSaving(false);
        }
      })();
    }
    wasFireRef.current = isFire;
  }, [detectOn, isFire, server, saving, T]);

  // joystick
  const RADIUS = 64, KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS, ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny); if (len > 1) { nx /= len; ny /= len; }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
      onPanResponderTerminate: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
    })
  ).current;

  const knobLeft = joy.x * RADIUS, knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* Linha: ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{T.espIp}</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder={T.placeholderIp}
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType="numeric"
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : T.test}</Text>
        </Pressable>

        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? T.ledOn : T.ledOff}</Text>
        </Pressable>

        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? T.pumpOn : T.pumpOff}</Text>
        </Pressable>
      </View>

      {/* Linha: Servidor IA + Detectar */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{T.server}</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder={T.placeholderServer}
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? (saving ? T.saving : T.detecting) : T.detect}</Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>{statusText}</Text>
      </View>

      {/* Banner de fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>{isFire ? T.fireOn(fireScore) : T.fireOff(fireScore)}</Text>
        </View>
      )}

      {/* Vídeo (sem flicker) */}
      <CrossfadeImage
        currentUri={currentFrameUri}
        nextUri={nextFrameUri}
        onNextLoadStart={onNextLoadStart}
        onNextShown={onNextShown}
      />

      {/* Joystick overlay (UI) */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              {
                width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R,
                transform: [{ translateX: knobLeft }, { translateY: knobTop }],
                opacity: dragging ? 1 : 0.9,
              },
            ]}
          />
          <Text style={styles.joyText}>x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}</Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },

  topbar: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 10,
    gap: 8,
    backgroundColor: PALETTE.card,
  },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: {
    backgroundColor: PALETTE.bg,
    color: "white",
    borderRadius: 8,
    paddingHorizontal: 10,
    paddingVertical: 8,
    borderWidth: 1,
    borderColor: "#1e2a4d",
    minWidth: 110,
  },
  btn: {
    backgroundColor: "#2b48ff",
    paddingHorizontal: 10,
    paddingVertical: 10,
    borderRadius: 10,
  },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },

  statusRow: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 8,
    gap: 8,
    backgroundColor: "#0f1730",
  },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },

  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },

  // Vídeo
  video: { flex: 1, backgroundColor: "black", width: "100%" },

  // Joystick
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});




Dicas rápidas




No app, Servidor = http://IP_DO_PC:8000.


Confirme /status mostra fps_in > 0 e hasFrame: true.


Se o Android físico não carregar: verifique Firewall do Windows (libere Python/uvicorn na rede privada).




Se quiser, posso te mandar também um requirements.txt para fixar versões das libs do servidor.











































Beleza — isso costuma acontecer porque o app está trocando a URL do próximo frame enquanto ele ainda está carregando, então nenhum chega a “finalizar” e nada aparece. Vamos consertar com um double-buffer com trava de carregamento:




só inicia o download do próximo frame quando o atual próximo terminar de carregar e for promovido;


mantém o último frame válido na tela o tempo todo (sem piscar);


se algo falhar, o servidor já devolve placeholder JPEG, então ainda aparece imagem.




Abaixo vai o CameraScreen.tsx completo com o ajuste (você só precisa trocar este arquivo). O seu server_heuristic.py anterior já serve; não precisa mudar.



app/screens/CameraScreen.tsx (atualizado – com trava de carregamento)


import { useEffect, useMemo, useRef, useState } from "react";
import {
  View, Text, TextInput, Pressable, StyleSheet, PanResponder,
  GestureResponderEvent, PanResponderGestureState, Image, Animated,
} from "react-native";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

// -------- Crossfade com trava: só muda quando o próximo terminar de carregar --------
function CrossfadeImage({
  currentUri,
  nextUri,
  onNextLoadStart,
  onNextShown,
}: {
  currentUri: string;
  nextUri: string;
  onNextLoadStart: () => void;
  onNextShown: () => void;
}) {
  const fade = useRef(new Animated.Value(0)).current;
  const [showNext, setShowNext] = useState(false);
  const lastNextRef = useRef<string>("");

  useEffect(() => {
    if (!nextUri || nextUri === lastNextRef.current) return;
    lastNextRef.current = nextUri;
    setShowNext(true);
  }, [nextUri]);

  return (
    <View style={{ flex: 1, backgroundColor: "black" }}>
      {/* base: frame atual */}
      {currentUri ? (
        <Image source={{ uri: currentUri }} style={{ flex: 1, width: "100%" }} resizeMode="contain" />
      ) : null}

      {/* topo: próximo frame (carrega por trás; quando pronto, faz fade in e promove) */}
      {showNext ? (
        <Animated.View style={[StyleSheet.absoluteFill, { opacity: fade }]}>
          <Image
            source={{ uri: nextUri }}
            style={{ flex: 1, width: "100%" }}
            resizeMode="contain"
            onLoadStart={onNextLoadStart}
            onError={() => {
              // mesmo com erro, promove para não travar a fila; o servidor envia placeholder JPEG,
              // então normalmente nem cai aqui, mas fica como “airbag”.
              onNextShown();
            }}
            onLoadEnd={() => {
              Animated.timing(fade, { toValue: 1, duration: 120, useNativeDriver: true }).start(() => {
                onNextShown();             // promove next -> current
                fade.setValue(0);          // zera para a próxima rodada
                setShowNext(false);
              });
            }}
          />
        </Animated.View>
      ) : null}
    </View>
  );
}

// -----------------------------------------------------------------------------------

const textsByLang = {
  pt: {
    espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Salvando...",
    statusFail: "Falha ao conectar. Confira o Wi-Fi e IP.",
    noVideo: "Sem vídeo (snapshot). Verifique o servidor.",
    fireOn: (s:number)=>`🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sem fogo • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento salvo em ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  en: {
    espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:",
    detect: "Detect", detecting: "Detecting", saving: "Saving...",
    statusFail: "Failed to connect. Check Wi-Fi/IP.",
    noVideo: "No video (snapshot). Check the server.",
    fireOn: (s:number)=>`🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s:number)=>`No fire • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Event saved at ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  es: {
    espIp: "ESP IP:", test: "Probar", ledOn: "LED ENC.", ledOff: "LED APAG.",
    pumpOn: "BOMBA ENC.", pumpOff: "BOMBA APAG.", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Guardando...",
    statusFail: "Error de conexión. Revisa Wi-Fi/IP.",
    noVideo: "Sin vídeo (snapshot). Revisa el servidor.",
    fireOn: (s:number)=>`🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sin fuego • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento guardado a las ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
};

const DEFAULT_IP = "192.168.4.1";                // ESP32-CAM (AP)
const DEFAULT_SERVER = "http://192.168.4.2:8000"; // Servidor FastAPI (PC)

export default function CameraScreen() {
  const { lang } = useLanguage();
  const T = textsByLang[lang];

  // conexões
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState("—");
  const [ledOn, setLedOn] = useState(false);
  const [pumpOn, setPumpOn] = useState(false);

  // detecção
  const [detectOn, setDetectOn] = useState(false);
  const [isFire, setIsFire] = useState(false);
  const [fireScore, setFireScore] = useState(0);

  // registro automático (cooldown)
  const [saving, setSaving] = useState(false);
  const lastSaveRef = useRef(0);
  const wasFireRef = useRef(false);
  const SAVE_COOLDOWN_MS = 5000;

  // vídeo (double buffer + trava)
  const [currentFrameUri, setCurrentFrameUri] = useState("");
  const [nextFrameUri, setNextFrameUri] = useState("");
  const loadingNextRef = useRef(false); // TRAVA: true enquanto o “próximo” está carregando

  // joystick (UI)
  const [joy, setJoy] = useState({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs do ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  // ===== ESP STATUS =====
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode} • led:${j.led ? "on" : "off"} • pump:${j.pump ? "on" : "off"}`);
    } catch {
      setStatusText(T.statusFail);
    } finally {
      setIsChecking(false);
    }
  }
  async function toggleLed() { try { const t = !ledOn; await fetch(ledUrl(t)); setLedOn(t); } catch { setStatusText(T.statusFail); } }
  async function togglePump() { try { const t = !pumpOn; await fetch(pumpUrl(t)); setPumpOn(t); } catch { setStatusText(T.statusFail); } }

  useEffect(() => { pingStatus(); const id = setInterval(pingStatus, 5000); return () => clearInterval(id); }, [statusUrl]);

  // ===== Vídeo via snapshots — com TRAVA de carregamento =====
  useEffect(() => {
    let stop = false;
    const FPS_INTERVAL = 300; // ~3 fps estáveis

    const tick = () => {
      if (stop) return;

      // só agenda novo frame se NÃO estiver carregando um “próximo”
      if (!loadingNextRef.current) {
        const url = `${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`;
        setNextFrameUri(url);
      }
      setTimeout(tick, FPS_INTERVAL);
    };

    // força um primeiro current (placeholder) para não ficar tela preta
    setCurrentFrameUri(`${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`);

    tick();
    return () => {
      stop = true;
    };
  }, [server]);

  function onNextLoadStart() {
    loadingNextRef.current = true;
  }
  function onNextShown() {
    if (nextFrameUri) {
      setCurrentFrameUri(nextFrameUri); // promove
    }
    loadingNextRef.current = false;     // libera para o próximo ciclo
  }

  // ===== DETECT =====
  useEffect(() => {
    if (!detectOn) return;
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server.replace(/\/+$/, "")}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && j.ok) {
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));
        }
      } catch {
        if (!stop) {
          setIsFire(false);
          setFireScore(0);
        }
      } finally {
        if (!stop) setTimeout(loop, 500);
      }
    };
    loop();
    return () => { stop = true; };
  }, [detectOn, server]);

  // ===== Registro automático (borda de subida) =====
  useEffect(() => {
    const rising = !wasFireRef.current && isFire;
    const now = Date.now();
    const cooldownOk = now - lastSaveRef.current >= SAVE_COOLDOWN_MS;

    if (detectOn && rising && cooldownOk && !saving) {
      (async () => {
        try {
          setSaving(true);
          await saveEventFromServer(server);
          lastSaveRef.current = Date.now();
          setStatusText(T.eventSavedAt(new Date(lastSaveRef.current).toLocaleString()));
        } catch {
          setStatusText(T.noVideo);
        } finally {
          setSaving(false);
        }
      })();
    }
    wasFireRef.current = isFire;
  }, [detectOn, isFire, server, saving, T]);

  // joystick
  const RADIUS = 64, KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS, ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny); if (len > 1) { nx /= len; ny /= len; }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
      onPanResponderTerminate: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
    })
  ).current;

  const knobLeft = joy.x * RADIUS, knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* Linha: ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{T.espIp}</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder={T.placeholderIp}
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType="numeric"
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : T.test}</Text>
        </Pressable>

        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? T.ledOn : T.ledOff}</Text>
        </Pressable>

        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? T.pumpOn : T.pumpOff}</Text>
        </Pressable>
      </View>

      {/* Linha: Servidor IA + Detectar */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{T.server}</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder={T.placeholderServer}
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? (saving ? T.saving : T.detecting) : T.detect}</Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>{statusText}</Text>
      </View>

      {/* Banner de fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>{isFire ? T.fireOn(fireScore) : T.fireOff(fireScore)}</Text>
        </View>
      )}

      {/* Vídeo (sem flicker) */}
      <CrossfadeImage
        currentUri={currentFrameUri}
        nextUri={nextFrameUri}
        onNextLoadStart={onNextLoadStart}
        onNextShown={onNextShown}
      />

      {/* Joystick overlay (UI) */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              {
                width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R,
                transform: [{ translateX: knobLeft }, { translateY: knobTop }],
                opacity: dragging ? 1 : 0.9,
              },
            ]}
          />
          <Text style={styles.joyText}>x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}</Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },

  topbar: {
    flexDirection: "row", alignItems: "center",
    paddingHorizontal: 12, paddingVertical: 10, gap: 8, backgroundColor: PALETTE.card,
  },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: {
    backgroundColor: PALETTE.bg, color: "white", borderRadius: 8,
    paddingHorizontal: 10, paddingVertical: 8, borderWidth: 1, borderColor: "#1e2a4d", minWidth: 110,
  },
  btn: { backgroundColor: "#2b48ff", paddingHorizontal: 10, paddingVertical: 10, borderRadius: 10 },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },

  statusRow: {
    flexDirection: "row", alignItems: "center",
    paddingHorizontal: 12, paddingVertical: 8, gap: 8, backgroundColor: "#0f1730",
  },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },

  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },

  // Vídeo
  video: { flex: 1, backgroundColor: "black", width: "100%" },

  // Joystick
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});




Checklist rápido (só para garantir):




Servidor no app deve ser http://IP_DO_PC:8000 (nada de localhost).


PC e celular na mesma rede (ou ambos no AP do ESP).


uvicorn rodando com --host 0.0.0.0.


Se Android físico: libere o Firewall do Windows para Python/uvicorn.


Teste no navegador do celular: http://IP_DO_PC:8000/snapshot deve abrir imagem (até mesmo o placeholder vermelho quando a câmera falhar).




Se ainda não aparecer, me diga:




o que mostra no navegador do celular ao abrir /snapshot,


e se o terminal do servidor imprime algum erro.






































Sim — eu ajustei o server_heuristic.py para ler o MJPEG manualmente (sem cv2.VideoCapture) e para sempre devolver um JPEG em /snapshot (com placeholder quando falhar), o que evita que o app quebre e elimina flicker quando combinado com o crossfade no app.


Aqui está o arquivo completo:


# server_heuristic.py
# FastAPI lendo MJPEG do ESP32-CAM manualmente (sem cv2.VideoCapture).
# /snapshot e /detect: abre :81/stream, extrai 1 JPEG e fecha.
# Em falha, /snapshot retorna um JPEG placeholder (status 200) para o app não quebrar.

import time
from typing import Optional, List
import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ---------- CONFIG ----------
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60
CONNECT_TIMEOUT = 3.0   # seg para conectar
READ_TIMEOUT = 3.0      # seg para ler primeiro frame
BOUNDARY = b"--frame"   # boundary usado no ESP
JPEG_QUALITY = 85

# ---------- FASTAPI ----------
app = FastAPI(title="HydroBot Fire (MJPEG manual)", version="0.3.1")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ---------- MJPEG READER ----------
def read_one_jpeg_from_mjpeg(ip: str) -> Optional[bytes]:
    """
    Abre http://<ip>:81/stream e extrai o PRIMEIRO JPEG usando o boundary '--frame'.
    Retorna bytes do JPEG ou None se falhar.
    """
    url = STREAM_URL_FMT.format(ip)
    t0 = time.time()
    try:
        with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
            if r.status_code != 200:
                return None

            buf = b""
            MAX_BYTES = 2_000_000
            while time.time() - t0 < READ_TIMEOUT:
                for chunk in r.iter_content(chunk_size=4096):
                    if not chunk:
                        continue
                    buf += chunk
                    if len(buf) > MAX_BYTES:
                        return None

                    # procura boundary
                    i = buf.find(BOUNDARY)
                    if i == -1:
                        continue

                    # pula \r\n após boundary (se houver)
                    hdr_start = i + len(BOUNDARY)
                    while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                        hdr_start += 2

                    # fim dos headers
                    headers_end = buf.find(b"\r\n\r\n", hdr_start)
                    if headers_end == -1:
                        continue

                    headers_bytes = buf[hdr_start:headers_end]
                    headers_text = headers_bytes.decode("latin1", errors="ignore")

                    content_length = None
                    for line in headers_text.split("\r\n"):
                        if line.lower().startswith("content-length:"):
                            try:
                                content_length = int(line.split(":", 1)[1].strip())
                            except:
                                pass
                            break

                    img_start = headers_end + 4
                    if content_length is not None:
                        # temos tamanho => espere tudo chegar
                        if len(buf) < img_start + content_length:
                            continue
                        return buf[img_start:img_start + content_length]
                    else:
                        # delimita até o próximo boundary
                        j = buf.find(BOUNDARY, img_start)
                        if j != -1:
                            jpeg = buf[img_start:j]
                            # tenta cortar até marcador de fim JPEG
                            end = jpeg.rfind(b"\xff\xd9")
                            if end != -1:
                                jpeg = jpeg[:end+2]
                            return jpeg
            return None
    except requests.exceptions.RequestException:
        return None

def decode_jpeg_to_bgr(jpeg_bytes: bytes):
    arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

# ---------- FIRE HEURISTIC ----------
def fire_score(frame) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    # faixa laranja/amarela comum em chamas
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
            continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

# ---------- HELPERS ----------
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)  # BGR
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ---------- ENDPOINTS ----------
@app.get("/status")
def status():
    return {"ok": True, "camera_ip": CAMERA_IP, "model": "mjpeg_manual", "threshold": SCORE_THRESHOLD}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        # devolve SEMPRE um JPEG para não quebrar o <Image>
        jpeg = placeholder_jpeg("NO FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        return {"ok": False, "error": "no frame"}
    frame = decode_jpeg_to_bgr(jpeg)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    s = fire_score(frame)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "threshold": SCORE_THRESHOLD,
        "boxes": fire_boxes(frame) if is_fire else [],
        "ts": int(time.time() * 1000),
    }



Como rodar (lembrando):


pip install fastapi uvicorn numpy opencv-python requests
uvicorn server_heuristic:app --host 0.0.0.0 --port 8000 --reload





No app, o campo Servidor deve apontar para http://IP_DO_SEU_PC:8000.


/snapshot sempre entrega um JPEG (placeholder se a câmera falhar).


Combine isso com o crossfade (double-buffer) que te mandei no CameraScreen.tsx para eliminar o flicker visual no app.


































Perfeito 👍 Vamos fazer isso organizado.

Como você está na Opção A (servidor intermediário), só precisa alterar dois arquivos:




app/screens/CameraScreen.tsx (no app React Native/Expo)


server_heuristic.py (no PC, FastAPI)





🔹 1. CameraScreen.tsx (com crossfade, sem flicker)


// app/screens/CameraScreen.tsx
import { useEffect, useMemo, useRef, useState } from "react";
import {
  View,
  Text,
  TextInput,
  Pressable,
  StyleSheet,
  PanResponder,
  GestureResponderEvent,
  PanResponderGestureState,
  Image,
  Animated,
} from "react-native";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

// crossfade para evitar flicker
function CrossfadeImage({
  currentUri,
  nextUri,
  onNextShown,
}: {
  currentUri: string;
  nextUri: string;
  onNextShown: () => void;
}) {
  const fade = useRef(new Animated.Value(0)).current;
  const [showNext, setShowNext] = useState(false);

  useEffect(() => {
    if (nextUri) setShowNext(true);
  }, [nextUri]);

  return (
    <View style={{ flex: 1, backgroundColor: "black" }}>
      {currentUri ? (
        <Image source={{ uri: currentUri }} style={{ flex: 1, width: "100%" }} resizeMode="contain" />
      ) : null}

      {showNext ? (
        <Animated.View style={[StyleSheet.absoluteFill, { opacity: fade }]}>
          <Image
            source={{ uri: nextUri }}
            style={{ flex: 1, width: "100%" }}
            resizeMode="contain"
            onLoadEnd={() => {
              Animated.timing(fade, { toValue: 1, duration: 120, useNativeDriver: true }).start(() => {
                onNextShown();
                fade.setValue(0);
                setShowNext(false);
              });
            }}
          />
        </Animated.View>
      ) : null}
    </View>
  );
}

export default function CameraScreen() {
  const { lang } = useLanguage();

  const texts = {
    pt: { espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF", pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:", detect: "Detectar", detecting: "Detectando", saving: "Salvando...", statusFail: "Falha ao conectar.", noVideo: "Sem vídeo.", fireOn: (s: number) => `🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s: number) => `Sem fogo • score ${s.toFixed(2)}`, },
    en: { espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF", pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:", detect: "Detect", detecting: "Detecting", saving: "Saving...", statusFail: "Failed to connect.", noVideo: "No video.", fireOn: (s: number) => `🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s: number) => `No fire • score ${s.toFixed(2)}`, },
    es: { espIp: "ESP IP:", test: "Probar", ledOn: "LED ENC.", ledOff: "LED APAG.", pumpOn: "BOMBA ENC.", pumpOff: "BOMBA APAG.", server: "Servidor:", detect: "Detectar", detecting: "Detectando", saving: "Guardando...", statusFail: "Error de conexión.", noVideo: "Sin video.", fireOn: (s: number) => `🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s: number) => `Sin fuego • score ${s.toFixed(2)}`, },
  }[lang];

  // conexões
  const [ip, setIp] = useState("192.168.4.1");
  const [server, setServer] = useState("http://192.168.4.2:8000");

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState<string>("—");
  const [ledOn, setLedOn] = useState(false);
  const [pumpOn, setPumpOn] = useState(false);

  // detecção
  const [detectOn, setDetectOn] = useState(false);
  const [isFire, setIsFire] = useState(false);
  const [fireScore, setFireScore] = useState(0);

  // vídeo (double buffer)
  const [currentFrameUri, setCurrentFrameUri] = useState("");
  const [nextFrameUri, setNextFrameUri] = useState("");

  // joystick
  const [joy, setJoy] = useState({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl);
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode}`);
    } catch {
      setStatusText(texts.statusFail);
    } finally {
      setIsChecking(false);
    }
  }

  async function toggleLed() {
    const t = !ledOn;
    await fetch(ledUrl(t));
    setLedOn(t);
  }

  async function togglePump() {
    const t = !pumpOn;
    await fetch(pumpUrl(t));
    setPumpOn(t);
  }

  // ping status
  useEffect(() => {
    pingStatus();
    const id = setInterval(pingStatus, 5000);
    return () => clearInterval(id);
  }, [statusUrl]);

  // snapshots sem flicker
  useEffect(() => {
    let stop = false;
    const FPS_INTERVAL = 300;
    const tick = () => {
      if (stop) return;
      const url = `${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`;
      setNextFrameUri(url);
      setTimeout(tick, FPS_INTERVAL);
    };
    tick();
    return () => {
      stop = true;
    };
  }, [server]);

  function handleNextShown() {
    if (nextFrameUri) setCurrentFrameUri(nextFrameUri);
  }

  // polling /detect
  useEffect(() => {
    if (!detectOn) return;
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server.replace(/\/+$/, "")}/detect`);
        const j = await r.json();
        if (!stop && j && j.ok) {
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));
        }
      } catch {
        if (!stop) {
          setIsFire(false);
          setFireScore(0);
        }
      } finally {
        if (!stop) setTimeout(loop, 500);
      }
    };
    loop();
    return () => {
      stop = true;
    };
  }, [detectOn, server]);

  // joystick
  const RADIUS = 64;
  const KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS;
        let ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny);
        if (len > 1) {
          nx /= len;
          ny /= len;
        }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => {
        setDragging(false);
        setJoy({ x: 0, y: 0 });
      },
      onPanResponderTerminate: () => {
        setDragging(false);
        setJoy({ x: 0, y: 0 });
      },
    })
  ).current;

  const knobLeft = joy.x * RADIUS;
  const knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* controles */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{texts.espIp}</Text>
        <TextInput value={ip} onChangeText={setIp} style={styles.input} />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : texts.test}</Text>
        </Pressable>
        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? texts.ledOn : texts.ledOff}</Text>
        </Pressable>
        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? texts.pumpOn : texts.pumpOff}</Text>
        </Pressable>
      </View>

      {/* servidor IA */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{texts.server}</Text>
        <TextInput value={server} onChangeText={setServer} style={[styles.input, { flex: 1 }]} />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? texts.detecting : texts.detect}</Text>
        </Pressable>
      </View>

      {/* status */}
      <Text style={styles.status}>{statusText}</Text>

      {/* banner de fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>{isFire ? texts.fireOn(fireScore) : texts.fireOff(fireScore)}</Text>
        </View>
      )}

      {/* vídeo */}
      <CrossfadeImage currentUri={currentFrameUri} nextUri={nextFrameUri} onNextShown={handleNextShown} />

      {/* joystick */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              { width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R, transform: [{ translateX: knobLeft }, { translateY: knobTop }] },
            ]}
          />
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },
  topbar: { flexDirection: "row", alignItems: "center", padding: 8, backgroundColor: PALETTE.card },
  statusRow: { flexDirection: "row", alignItems: "center", padding: 8, backgroundColor: "#0f1730" },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: { backgroundColor: PALETTE.bg, color: "white", borderRadius: 8, paddingHorizontal: 8, paddingVertical: 6, borderWidth: 1, borderColor: "#1e2a4d", minWidth: 110 },
  btn: { backgroundColor: "#2b48ff", paddingHorizontal: 10, paddingVertical: 8, borderRadius: 8 },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },
  status: { color: "#cbd5ff", fontSize: 12, margin: 6 },
  fireBanner: { alignItems: "center", padding: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },
  video: { flex: 1, backgroundColor: "black", width: "100%" },
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
});




🔹 2. server_heuristic.py (com placeholder para nunca quebrar)


Esse já é o que te mandei antes, com requests e fallback JPEG vermelho. Não precisa mudar se já está usando.


👉 Só garanta que /snapshot sempre retorna imagem (não JSON), mesmo quando não conseguir frame.



Assim, o app mostra a câmera sem piscar: sempre mantém o último frame até o próximo estar pronto, com crossfade suave.


Quer que eu adapte também o DashboardScreen para usar esse mesmo esquema de snapshot sem piscar quando mostrar as pré-visualizações?


































Perfeito — segue a Opção A completa e arrumada:



1) server_heuristic.py (substitua o seu por este)


# server_heuristic.py
# FastAPI lendo MJPEG do ESP32-CAM manualmente (sem cv2.VideoCapture).
# Em /snapshot e /detect abrimos o :81/stream, extraímos 1 JPEG e fechamos.
# Em caso de falha, devolvemos um JPEG "placeholder" com status 200 para o app não quebrar.

import time
from typing import Optional, List
import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ---------- CONFIG ----------
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60
CONNECT_TIMEOUT = 3.0   # s para conectar
READ_TIMEOUT = 3.0      # s para ler o primeiro frame
BOUNDARY = b"--frame"   # seu ESP usa "frame"
JPEG_QUALITY = 85

# ---------- FASTAPI ----------
app = FastAPI(title="HydroBot Fire (MJPEG manual)", version="0.3.1")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ---------- MJPEG READER ----------
def read_one_jpeg_from_mjpeg(ip: str) -> Optional[bytes]:
    """
    Abre http://<ip>:81/stream e extrai o PRIMEIRO JPEG via boundary '--frame'.
    Retorna bytes do JPEG ou None se falhar.
    """
    url = STREAM_URL_FMT.format(ip)
    t0 = time.time()
    try:
        with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
            if r.status_code != 200:
                return None

            buf = b""
            MAX_BYTES = 2_000_000
            while time.time() - t0 < READ_TIMEOUT:
                for chunk in r.iter_content(chunk_size=4096):
                    if not chunk:
                        continue
                    buf += chunk
                    if len(buf) > MAX_BYTES:
                        return None

                    # procura um boundary
                    i = buf.find(BOUNDARY)
                    if i == -1:
                        continue

                    # pula \r\n após boundary (se houver)
                    hdr_start = i + len(BOUNDARY)
                    while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                        hdr_start += 2

                    # fim de headers
                    headers_end = buf.find(b"\r\n\r\n", hdr_start)
                    if headers_end == -1:
                        # ainda não temos todos os headers
                        continue

                    headers_bytes = buf[hdr_start:headers_end]
                    headers_text = headers_bytes.decode("latin1", errors="ignore")

                    content_length = None
                    for line in headers_text.split("\r\n"):
                        if line.lower().startswith("content-length:"):
                            try:
                                content_length = int(line.split(":", 1)[1].strip())
                            except:
                                pass
                            break

                    img_start = headers_end + 4
                    if content_length is not None:
                        # temos tamanho => garanta que chegou tudo
                        if len(buf) < img_start + content_length:
                            continue
                        return buf[img_start:img_start + content_length]
                    else:
                        # sem Content-Length => delimite pelo próximo boundary
                        j = buf.find(BOUNDARY, img_start)
                        if j != -1:
                            jpeg = buf[img_start:j]
                            # tente cortar até marcador de fim JPEG
                            end = jpeg.rfind(b"\xff\xd9")
                            if end != -1:
                                jpeg = jpeg[:end+2]
                            return jpeg
            return None
    except requests.exceptions.RequestException:
        return None

def decode_jpeg_to_bgr(jpeg_bytes: bytes):
    arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

# ---------- FIRE HEURISTIC ----------
def fire_score(frame) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    # faixa laranja/amarela comum em chamas
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
            continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

# ---------- HELPERS ----------
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)  # BGR
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ---------- ENDPOINTS ----------
@app.get("/status")
def status():
    return {"ok": True, "camera_ip": CAMERA_IP, "model": "mjpeg_manual", "threshold": SCORE_THRESHOLD}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        # devolve JPEG de placeholder com 200 para o <Image> não disparar onError
        jpeg = placeholder_jpeg("NO FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        return {"ok": False, "error": "no frame"}
    frame = decode_jpeg_to_bgr(jpeg)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    s = fire_score(frame)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "threshold": SCORE_THRESHOLD,
        "boxes": fire_boxes(frame) if is_fire else [],
        "ts": int(time.time() * 1000),
    }





Dependências (uma vez):


pip install fastapi uvicorn numpy opencv-python requests







Execução:


uvicorn server_heuristic:app --host 0.0.0.0 --port 8000 --reload



Depois, no celular (mesma rede do PC), teste no navegador:




http://IP_DO_PC:8000/status  → deve mostrar JSON


http://IP_DO_PC:8000/snapshot → deve mostrar uma imagem (placeholder se a câmera falhar)







2) app/screens/CameraScreen.tsx (use este arquivo)


// app/screens/CameraScreen.tsx
import { useEffect, useMemo, useRef, useState } from "react";
import {
  View, Text, TextInput, Pressable, StyleSheet, PanResponder,
  GestureResponderEvent, PanResponderGestureState, Image,
} from "react-native";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

const texts = {
  pt: {
    espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Salvando...",
    statusFail: "Falha ao conectar. Confira o Wi-Fi HYDROBOT-CAM e o IP.",
    noVideo: "Sem vídeo (snapshot). Verifique o servidor.",
    fireOn: (s:number)=>`🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sem fogo • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento salvo em ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  en: {
    espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:",
    detect: "Detect", detecting: "Detecting", saving: "Saving...",
    statusFail: "Failed to connect. Check HYDROBOT-CAM Wi-Fi and IP.",
    noVideo: "No video (snapshot). Check the server.",
    fireOn: (s:number)=>`🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s:number)=>`No fire • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Event saved at ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
  es: {
    espIp: "ESP IP:", test: "Probar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Guardando...",
    statusFail: "Fallo de conexión. Verifica el Wi-Fi HYDROBOT-CAM y el IP.",
    noVideo: "Sin vídeo (snapshot). Revisa el servidor.",
    fireOn: (s:number)=>`🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sin fuego • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento guardado a las ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
  },
};

const DEFAULT_IP = "192.168.4.1";                // ESP32-CAM
const DEFAULT_SERVER = "http://192.168.4.2:8000"; // PC rodando FastAPI

export default function CameraScreen() {
  const { lang } = useLanguage();
  const T = texts[lang];

  // conexões
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState<string>("—");
  const [ledOn, setLedOn] = useState<boolean>(false);
  const [pumpOn, setPumpOn] = useState<boolean>(false);

  // detecção
  const [detectOn, setDetectOn] = useState<boolean>(false);
  const [isFire, setIsFire] = useState<boolean>(false);
  const [fireScore, setFireScore] = useState<number>(0);

  // controle de registro (cooldown)
  const [saving, setSaving] = useState(false);
  const lastSaveRef = useRef<number>(0);
  const wasFireRef = useRef<boolean>(false);
  const SAVE_COOLDOWN_MS = 5000;

  // “vídeo” (snapshots do servidor)
  const [frameUri, setFrameUri] = useState<string>("");

  // joystick (UI)
  const [joy, setJoy] = useState<{ x: number; y: number }>({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs do ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  // ===== ESP STATUS =====
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode} • led:${j.led ? "on" : "off"} • pump:${j.pump ? "on" : "off"}`);
    } catch {
      setStatusText(T.statusFail);
    } finally {
      setIsChecking(false);
    }
  }
  async function toggleLed() { try { const t = !ledOn; await fetch(ledUrl(t)); setLedOn(t); } catch { setStatusText(T.statusFail); } }
  async function togglePump() { try { const t = !pumpOn; await fetch(pumpUrl(t)); setPumpOn(t); } catch { setStatusText(T.statusFail); } }

  useEffect(() => { pingStatus(); const id = setInterval(pingStatus, 5000); return () => clearInterval(id); }, [statusUrl]);

  // ===== Vídeo via snapshots =====
  useEffect(() => {
    let stop = false;
    const tick = () => {
      if (stop) return;
      // aponta para o servidor FastAPI (PC), não para o ESP
      setFrameUri(`${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`);
      setTimeout(tick, 250); // ~4 fps
    };
    tick();
    return () => { stop = true; };
  }, [server]);

  // ===== DETECT =====
  useEffect(() => {
    if (!detectOn) return;
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server.replace(/\/+$/, "")}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && j.ok) { setIsFire(!!j.isFire); setFireScore(Number(j.score || 0)); }
      } catch {
        if (!stop) { setIsFire(false); setFireScore(0); }
      } finally {
        if (!stop) setTimeout(loop, 500);
      }
    };
    loop();
    return () => { stop = true; };
  }, [detectOn, server]);

  // ===== Registro de evento (rising edge) =====
  useEffect(() => {
    const risingEdge = !wasFireRef.current && isFire;
    const now = Date.now();
    const cooldownOk = now - lastSaveRef.current >= SAVE_COOLDOWN_MS;
    if (detectOn && risingEdge && cooldownOk && !saving) {
      (async () => {
        try {
          setSaving(true);
          await saveEventFromServer(server);
          lastSaveRef.current = Date.now();
          setStatusText(T.eventSavedAt(new Date(lastSaveRef.current).toLocaleString()));
        } catch {
          setStatusText(T.noVideo);
        } finally {
          setSaving(false);
        }
      })();
    }
    wasFireRef.current = isFire;
  }, [detectOn, isFire, server, saving, T]);

  // joystick
  const RADIUS = 64, KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS, ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny); if (len > 1) { nx /= len; ny /= len; }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
      onPanResponderTerminate: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
    })
  ).current;

  const knobLeft = joy.x * RADIUS, knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* Linha: ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{T.espIp}</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder={T.placeholderIp}
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType="numeric"
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : T.test}</Text>
        </Pressable>

        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? T.ledOn : T.ledOff}</Text>
        </Pressable>

        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? T.pumpOn : T.pumpOff}</Text>
        </Pressable>
      </View>

      {/* Linha: Servidor IA + Detectar */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{T.server}</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder={T.placeholderServer}
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? (saving ? T.saving : T.detecting) : T.detect}</Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>{statusText}</Text>
      </View>

      {/* Banner de fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>
            {isFire ? T.fireOn(fireScore) : T.fireOff(fireScore)}
          </Text>
        </View>
      )}

      {/* “Vídeo” via snapshots do servidor */}
      <Image
        source={{ uri: frameUri }}
        style={styles.video}
        resizeMode="contain"
        onError={() => { setStatusText(T.noVideo); }}
      />

      {/* Joystick overlay (UI) */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              {
                width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R,
                transform: [{ translateX: knobLeft }, { translateY: knobTop }],
                opacity: dragging ? 1 : 0.9,
              },
            ]}
          />
          <Text style={styles.joyText}>x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}</Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },
  topbar: { flexDirection: "row", alignItems: "center", paddingHorizontal: 12, paddingVertical: 10, gap: 8, backgroundColor: PALETTE.card },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: { backgroundColor: PALETTE.bg, color: "white", borderRadius: 8, paddingHorizontal: 10, paddingVertical: 8, borderWidth: 1, borderColor: "#1e2a4d", minWidth: 110 },
  btn: { backgroundColor: "#2b48ff", paddingHorizontal: 10, paddingVertical: 10, borderRadius: 10 },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },
  statusRow: { flexDirection: "row", alignItems: "center", paddingHorizontal: 12, paddingVertical: 8, gap: 8, backgroundColor: "#0f1730" },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },
  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },
  video: { flex: 1, backgroundColor: "black", width: "100%" },
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});




Lembretes essenciais




No app, no campo Servidor, coloque http://IP_DO_PC:8000 (não use localhost).


PC e celular precisam estar na mesma rede (ou ambos no AP do ESP, com IP fixo no PC, ex.: 192.168.4.2).


O endpoint /snapshot agora sempre retorna um JPEG (placeholder quando falhar), então o <Image> não fica em erro.




Se quiser, posso te enviar também um pequeno script para testar o MJPEG do ESP via linha de comando e validar a leitura do primeiro frame.
















































// app/screens/CameraScreen.tsx
import { useEffect, useMemo, useRef, useState } from "react";
import {
  View, Text, TextInput, Pressable, StyleSheet, PanResponder,
  GestureResponderEvent, PanResponderGestureState, Image, Platform,
} from "react-native";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

const texts = {
  pt: {
    espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Salvando...",
    statusFail: "Falha ao conectar. Confira o Wi-Fi HYDROBOT-CAM e o IP.",
    noVideo: "Sem vídeo. Verifique o /snapshot do ESP.",
    fireOn: (s:number)=>`🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sem fogo • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento salvo em ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    needServer: "Defina o campo Servidor para detectar fogo.",
    usingEspSnapshot: "Exibindo vídeo via /snapshot do ESP (porta 80).",
  },
  en: {
    espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:",
    detect: "Detect", detecting: "Detecting", saving: "Saving...",
    statusFail: "Failed to connect. Check HYDROBOT-CAM Wi-Fi and IP.",
    noVideo: "No video. Check ESP /snapshot.",
    fireOn: (s:number)=>`🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s:number)=>`No fire • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Event saved at ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    needServer: "Set the Server field to run fire detection.",
    usingEspSnapshot: "Showing video via ESP /snapshot (port 80).",
  },
  es: {
    espIp: "ESP IP:", test: "Probar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Guardando...",
    statusFail: "Fallo de conexión. Verifica el Wi-Fi HYDROBOT-CAM y el IP.",
    noVideo: "Sin vídeo. Revisa /snapshot del ESP.",
    fireOn: (s:number)=>`🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sin fuego • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento guardado a las ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    needServer: "Define el Servidor para detectar fuego.",
    usingEspSnapshot: "Mostrando vídeo vía /snapshot del ESP (puerto 80).",
  },
};

const DEFAULT_IP = "192.168.4.1";
const DEFAULT_SERVER = ""; // vazio => detecção desligada

export default function CameraScreen() {
  const { lang } = useLanguage();
  const T = texts[lang];

  // conexões
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState<string>("—");
  const [ledOn, setLedOn] = useState<boolean>(false);
  const [pumpOn, setPumpOn] = useState<boolean>(false);

  // detecção
  const [detectOn, setDetectOn] = useState<boolean>(false);
  const [isFire, setIsFire] = useState<boolean>(false);
  const [fireScore, setFireScore] = useState<number>(0);

  // registro
  const [saving, setSaving] = useState(false);
  const lastSaveRef = useRef<number>(0);
  const wasFireRef = useRef<boolean>(false);
  const SAVE_COOLDOWN_MS = 5000;

  // vídeo via /snapshot (ESP)
  const [frameUri, setFrameUri] = useState<string>("");
  const fetchBusyRef = useRef<boolean>(false); // evita concorrência de updates
  const stopVideoRef = useRef<boolean>(false);

  // joystick (UI)
  const [joy, setJoy] = useState<{ x: number; y: number }>({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // bases
  const espBase = useMemo(() => `http://${ip}`, [ip]);
  const serverBase = useMemo(() => (server && server.trim()) ? server.trim().replace(/\/+$/,"") : "", [server]);

  // URLs do ESP
  const statusUrl = useMemo(() => `${espBase}/status`, [espBase]);
  const ledUrl    = useMemo(() => (on: boolean) => `${espBase}/led?on=${on ? "1" : "0"}`, [espBase]);
  const pumpUrl   = useMemo(() => (on: boolean) => `${espBase}/pump?on=${on ? "1" : "0"}`, [espBase]);

  // ===== ESP STATUS =====
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode} • led:${j.led ? "on" : "off"} • pump:${j.pump ? "on" : "off"}`);
    } catch {
      setStatusText(T.statusFail);
    } finally {
      setIsChecking(false);
    }
  }
  async function toggleLed() { try { const t = !ledOn; await fetch(ledUrl(t)); setLedOn(t); } catch { setStatusText(T.statusFail); } }
  async function togglePump() { try { const t = !pumpOn; await fetch(pumpUrl(t)); setPumpOn(t); } catch { setStatusText(T.statusFail); } }

  useEffect(() => { pingStatus(); const id = setInterval(pingStatus, 5000); return () => clearInterval(id); }, [statusUrl]);

  // ===== VÍDEO POR SNAPSHOT DO ESP (sem MJPEG, sem WebView) =====
  // Evita disputar o /stream com o servidor. Apenas puxa /snapshot do ESP em loop.
  useEffect(() => {
    setStatusText(T.usingEspSnapshot);
    stopVideoRef.current = false;
    const TICK_MS = 250;

    const tick = () => {
      if (stopVideoRef.current) return;
      if (!fetchBusyRef.current) {
        fetchBusyRef.current = true;
        // Apenas atualiza o URI do <Image>; o RN buscará a imagem.
        setFrameUri(`${espBase}/snapshot?ts=${Date.now()}`);
        // Liberamos o "busy" logo após, pois o <Image> vai fazer a requisição
        // sem bloquear o JS thread.
        fetchBusyRef.current = false;
      }
      setTimeout(tick, TICK_MS);
    };

    tick();
    return () => { stopVideoRef.current = true; };
  }, [espBase, T.usingEspSnapshot]);

  // ===== DETECT (sempre no SERVIDOR externo) =====
  useEffect(() => {
    if (!detectOn) return;
    if (!serverBase) {
      setStatusText(T.needServer);
      setIsFire(false);
      setFireScore(0);
      return;
    }
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${serverBase}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && (j.ok === undefined || j.ok)) {
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));
        }
      } catch {
        if (!stop) { setIsFire(false); setFireScore(0); }
      } finally {
        if (!stop) setTimeout(loop, 500);
      }
    };
    loop();
    return () => { stop = true; };
  }, [detectOn, serverBase, T.needServer]);

  // ===== Registro (rising edge) — usa o SERVIDOR =====
  useEffect(() => {
    const risingEdge = !wasFireRef.current && isFire;
    const now = Date.now();
    const cooldownOk = now - lastSaveRef.current >= SAVE_COOLDOWN_MS;
    if (detectOn && risingEdge && cooldownOk && !saving) {
      if (!serverBase) {
        setStatusText(T.needServer);
      } else {
        (async () => {
          try {
            setSaving(true);
            await saveEventFromServer(serverBase);
            lastSaveRef.current = Date.now();
            setStatusText(T.eventSavedAt(new Date(lastSaveRef.current).toLocaleString()));
          } catch {
            setStatusText(T.noVideo);
          } finally {
            setSaving(false);
          }
        })();
      }
    }
    wasFireRef.current = isFire;
  }, [detectOn, isFire, serverBase, saving, T]);

  // joystick
  const RADIUS = 64, KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS, ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny); if (len > 1) { nx /= len; ny /= len; }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
      onPanResponderTerminate: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
    })
  ).current;

  const knobLeft = joy.x * RADIUS, knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{T.espIp}</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder={T.placeholderIp}
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType={Platform.select({ ios: "numbers-and-punctuation", android: "numeric", default: "default" })}
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : T.test}</Text>
        </Pressable>
        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? T.ledOn : T.ledOff}</Text>
        </Pressable>
        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? T.pumpOn : T.pumpOff}</Text>
        </Pressable>
      </View>

      {/* Servidor + Detect */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{T.server}</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder={T.placeholderServer}
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn(v => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>
            {detectOn ? (saving ? T.saving : T.detecting) : T.detect}
          </Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>{statusText}</Text>
      </View>

      {/* Banner fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>
            {isFire ? T.fireOn(fireScore) : T.fireOff(fireScore)}
          </Text>
        </View>
      )}

      {/* VÍDEO: sempre via /snapshot do ESP */}
      <Image
        source={{ uri: frameUri }}
        style={styles.video}
        resizeMode="contain"
        onError={() => { setStatusText(T.noVideo); }}
      />

      {/* Joystick */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View style={[styles.joyKnob, { width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R, transform: [{ translateX: knobLeft }, { translateY: knobTop }], opacity: dragging ? 1 : 0.9 }]} />
          <Text style={styles.joyText}>x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}</Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },
  topbar: { flexDirection: "row", alignItems: "center", paddingHorizontal: 12, paddingVertical: 10, gap: 8, backgroundColor: PALETTE.card },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: { backgroundColor: PALETTE.bg, color: "white", borderRadius: 8, paddingHorizontal: 10, paddingVertical: 8, borderWidth: 1, borderColor: "#1e2a4d", minWidth: 110 },
  btn: { backgroundColor: "#2b48ff", paddingHorizontal: 10, paddingVertical: 10, borderRadius: 10 },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },
  statusRow: { flexDirection: "row", alignItems: "center", paddingHorizontal: 12, paddingVertical: 8, gap: 8, backgroundColor: "#0f1730" },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },
  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },
  video: { flex: 1, backgroundColor: "black", width: "100%" },
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});










import cv2, time, threading, numpy as np
from typing import Optional, List
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60

app = FastAPI(title="HydroBot Fire (Heuristic)", version="0.1.0")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

class CameraReader:
    def __init__(self, ip: str):
        self.ip = ip; self._cap=None; self._lock=threading.Lock()
        self._last=None; self._fps=0.0; self._run=False; self._th=None
    def start(self):
        if self._run: return
        self._run=True; self._th=threading.Thread(target=self._loop,daemon=True); self._th.start()
    def stop(self):
        self._run=False
        if self._th: self._th.join(timeout=2.0)
        if self._cap: self._cap.release(); self._cap=None
    def set_ip(self, ip:str):
        if ip==self.ip: return
        self.ip=ip; self.stop(); self.start()
    def _open(self):
        cap=cv2.VideoCapture(STREAM_URL_FMT.format(self.ip)); cap.set(cv2.CAP_PROP_BUFFERSIZE,1); return cap
    def _loop(self):
        self._cap=self._open()
        for _ in range(10):
            if self._cap and self._cap.isOpened(): break
            time.sleep(0.6); self._cap=self._open()
        frames=0; last=time.time()
        while self._run:
            if not self._cap or not self._cap.isOpened(): time.sleep(0.3); continue
            ok,frame=self._cap.read()
            if not ok or frame is None: time.sleep(0.03); continue
            with self._lock: self._last=frame
            frames+=1; now=time.time()
            if now-last>=1.0: self._fps=frames/(now-last); frames=0; last=now
    def get_latest(self): 
        with self._lock: return None if self._last is None else self._last.copy()
    def get_fps(self): return self._fps

reader=CameraReader(CAMERA_IP); reader.start()

class ConfigIn(BaseModel): camera_ip: str

def fire_score(frame):
    hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    mask=cv2.inRange(hsv,(5,80,120),(40,255,255))
    ratio=float(np.count_nonzero(mask))/float(mask.size)
    v_mean=float(np.mean(hsv[...,2]))/255.0
    return min(1.0, ratio*4.0 + v_mean*0.2)

def fire_boxes(frame):
    h,w=frame.shape[:2]; scl=min(640,w)
    rs=cv2.resize(frame,(scl,int(h*scl/w))); hsv=cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask=cv2.inRange(hsv,(5,80,120),(40,255,255))
    k=np.ones((5,5),np.uint8)
    mask=cv2.morphologyEx(mask, cv2.MORPH_OPEN, k,1); mask=cv2.morphologyEx(mask, cv2.MORPH_DILATE, k,1)
    cnts,_=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    rx=w/float(scl); ry=h/float(rs.shape[0]); boxes=[]
    for c in cnts:
        x,y,bw,bh=cv2.boundingRect(c)
        if bw*bh<600: continue
        boxes.append([int(x*rx),int(y*ry),int(bw*rx),int(bh*ry)])
    return boxes

@app.get("/status")
def status(): 
    return {"ok":True,"camera_ip":reader.ip,"fps":round(reader.get_fps(),2),"has_frame":reader.get_latest() is not None,"model":"heuristic","ts":int(time.time()*1000)}

@app.post("/config")
def set_config(cfg:ConfigIn):
    reader.set_ip(cfg.camera_ip); return {"ok":True,"camera_ip":reader.ip}

@app.get("/snapshot")
def snapshot():
    f=reader.get_latest()
    if f is None: return {"ok":False,"error":"no frame"}
    ok,buf=cv2.imencode(".jpg",f,[int(cv2.IMWRITE_JPEG_QUALITY),85])
    if not ok: return {"ok":False,"error":"encode failed"}
    return Response(content=buf.tobytes(),media_type="image/jpeg")

@app.get("/detect")
def detect():
    f=reader.get_latest()
    if f is None: return {"ok":False,"error":"no frame"}
    s=fire_score(f); is_fire=bool(s>=SCORE_THRESHOLD)
    return {"ok":True,"isFire":is_fire,"score":round(float(s),3),"boxes":(fire_boxes(f) if is_fire else []),"threshold":SCORE_THRESHOLD,"ts":int(time.time()*1000)}
