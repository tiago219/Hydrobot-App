
Vale — o app está ok; o problema é o servidor não está retornando objetos. Isso costuma ser por par (prototxt/pesos) diferente ou pré-processamento (swapRB) que não bate com seu caffemodel. Vou te passar:




server_heuristic.py completo com:



suporte a vários nomes de modelo (os que você tem na pasta),


ajuste de threshold por query,


fallback automático de pré-processamento (tenta swapRB=False e, se nada sair, troca pra swapRB=True),


endpoints de debug: /objects_raw, /objects_frame e /debug_models (mostram proto, weights, conf_max, etc).






Passo a passo de teste.






Não mexe na detecção de fogo; só acrescentei a parte de objetos e debug.





Substitua seu hydrobot-server/server_heuristic.py por este (COMPLETO)


# server_heuristic.py
# FOGO (heurístico, inalterado) + OBJETOS (MobileNet-SSD Caffe) com debug
import os, time, threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ===================== CONFIG STREAM =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

JPEG_QUALITY = 85

# ===================== FOGO (igual ao seu) =====================
HSV_LOW = (8, 80, 120)
HSV_HIGH = (40, 255, 255)
RED_DELTA = 15

DETECTOR_MAX_FPS = 14.0
HYST_HIGH = 0.18
HYST_LOW  = 0.15
VOTE_WINDOW = 7
VOTE_NEED   = 4
EMA_ALPHA   = 0.25
MIN_BLOB_AREA = 1200
KERNEL_SZ = 5

MOTION_THRESH = 22
MOTION_DILATE_ITERS = 1

PERSIST_CONSEC = 2
IOU_MIN = 0.15

MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== OBJETOS (MobileNet-SSD) =====================
OBJECTS_MAX_FPS = 10.0
OBJ_CONF_THRESH = 0.25       # pode trocar em /objects?conf=0.15
OBJ_NMS_THRESH  = 0.45
OBJ_CLASSES = [
    "background","aeroplane","bicycle","bird","boat","bottle","bus","car","cat","chair",
    "cow","diningtable","dog","horse","motorbike","person","pottedplant","sheep","sofa","train","tvmonitor"
]
ANIMAL_CLASSES = {"bird","cat","cow","dog","horse","sheep"}

DNN_IN_SIZE = (300, 300)
DNN_SCALE   = 0.007843
DNN_MEAN    = 127.5

# aceita os nomes que você tem na pasta
DNN_PROTOTXT_CANDIDATES = [
    "./models/MobileNetSSD_deploy.prototxt",
    "./models/MobileNetSSD_deploy.prototxt.txt",
    "./models/deploy.prototxt",
    "./models/voc/MobileNetSSD_test.prototxt",
]
DNN_CAFFE_WEIGHTS_CANDIDATES = [
    "./models/MobileNetSSD_deploy.caffemodel",
    "./models/mobilenet_iter_73000.caffemodel",
]

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire + Objects", version="1.4.0")
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP
        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip: self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5); continue
                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()
                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set(): break
                        if not chunk: continue
                        buf += chunk
                        if len(buf) > MAX_BYTES: buf = b""
                        i = buf.find(BOUNDARY)
                        if i == -1: continue
                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2
                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1: continue
                        headers_bytes = buf[hdr_start:headers_end]
                        content_length = None
                        for line in headers_bytes.split(b"\r\n"):
                            if line.lower().startswith(b"content-length:"):
                                try: content_length = int(line.split(b":", 1)[1].strip())
                                except: pass
                                break
                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length: continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue
                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None: return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms: return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== FUNÇÕES VISÃO (fogo) =====================
def rgb_red_dominance_mask(frame_bgr: np.ndarray, delta: int = RED_DELTA) -> np.ndarray:
    b, g, r = cv2.split(frame_bgr)
    mask = (r.astype(np.int16) > (g.astype(np.int16) + delta)) & (r.astype(np.int16) > (b.astype(np.int16) + delta))
    return (mask.astype(np.uint8)) * 255

def hsv_fire_mask(frame_bgr: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
    lower = np.array(HSV_LOW, dtype=np.uint8)
    upper = np.array(HSV_HIGH, dtype=np.uint8)
    return cv2.inRange(hsv, lower, upper)

def skin_mask_ycrcb(frame_bgr: np.ndarray) -> np.ndarray:
    ycrcb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    skin = cv2.inRange(ycrcb, (0, 133, 77), (255, 173, 127))
    dark = cv2.threshold(y, 60, 255, cv2.THRESH_BINARY)[1]
    skin = cv2.bitwise_and(skin, dark)
    return skin

def iou(a: Tuple[int,int,int,int], b: Tuple[int,int,int,int]) -> float:
    ax, ay, aw, ah = a; bx, by, bw, bh = b
    ax2, ay2 = ax + aw, ay + ah; bx2, by2 = bx + bw, by + bh
    ix1, iy1 = max(ax, bx), max(ay, by)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)
    inter = iw * ih; union = aw*ah + bw*bh - inter
    return float(inter) / float(union) if union > 0 else 0.0

def boxes_from_mask(mask_bin: np.ndarray, min_area: int = MIN_BLOB_AREA) -> List[List[int]]:
    k = np.ones((KERNEL_SZ, KERNEL_SZ), np.uint8)
    m = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, k, 1)
    m = cv2.morphologyEx(m, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if w * h >= min_area:
            boxes.append([x, y, w, h])
    return boxes

# ===================== DETECTOR CONTÍNUO (FOGO) =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._persist_hits = 0
        self._last_main_box: Optional[Tuple[int,int,int,int]] = None

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            mask_hsv = hsv_fire_mask(frame)
            mask_skin = skin_mask_ycrcb(frame)
            mask_red  = rgb_red_dominance_mask(frame)

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            stable = cv2.bitwise_and(mask_hsv, cv2.bitwise_not(mask_skin))
            stable = cv2.bitwise_and(stable, cv2.bitwise_not(motion_mask))

            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            V = hsv[..., 2]
            bright = cv2.threshold(V, 200, 255, cv2.THRESH_BINARY)[1]
            red_boost = cv2.bitwise_and(mask_red, bright)
            combined = cv2.bitwise_or(stable, red_boost)

            ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
            v_mean = float(np.mean(V)) / 255.0
            score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)

            ratio_combined = float(np.count_nonzero(combined)) / float(combined.size)
            score_combined = min(1.0, ratio_combined * 5.0 + v_mean * 0.1)

            ema = score_combined if self._score_ema == 0.0 else (EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema)

            boxes = boxes_from_mask(combined, MIN_BLOB_AREA)

            main_box = None
            if boxes:
                areas = [w*h for (_,_,w,h) in boxes]
                main_box = boxes[int(np.argmax(areas))]
                if self._last_main_box is not None:
                    if iou(tuple(main_box), tuple(self._last_main_box)) >= IOU_MIN:
                        self._persist_hits += 1
                    else:
                        self._persist_hits = 1
                else:
                    self._persist_hits = 1
            else:
                self._persist_hits = 0
            self._last_main_box = tuple(main_box) if main_box is not None else None

            if ema >= HYST_HIGH and self._persist_hits >= PERSIST_CONSEC:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                guess = (1 if (len(self._votes) > 0 and self._votes[-1] == 1 and self._persist_hits >= PERSIST_CONSEC) else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes if self._is_fire else []
                self._last_result_ts = int(time.time() * 1000)

            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
                "persist": {"hits": self._persist_hits, "need": PERSIST_CONSEC, "iou_min": IOU_MIN},
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
            }

detector = Detector(grabber)
detector.start()

# ===================== OBJETOS =====================
class ObjectsDetector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self.backend = "mobilenet-ssd"
        self.net = None
        self.ok = False
        self.proto_path = None
        self.weights_path = None

        # debug
        self._last_conf_max = 0.0
        self.swap_rb = False           # fallback automático
        self._nohit_streak = 0

        # FPS
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

        self._last: Dict[str, Any] = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0}

        self._load_net()

    def _load_net(self):
        for p in DNN_PROTOTXT_CANDIDATES:
            if os.path.exists(p): self.proto_path = p; break
        for w in DNN_CAFFE_WEIGHTS_CANDIDATES:
            if os.path.exists(w): self.weights_path = w; break
        if not self.proto_path:
            self.ok = False; self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0, "error": "prototxt not found"}; return
        if not self.weights_path:
            self.ok = False; self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0, "error": "caffemodel not found"}; return
        try:
            self.net = cv2.dnn.readNetFromCaffe(self.proto_path, self.weights_path)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            self.ok = True
        except Exception as e:
            self.ok = False
            self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0, "error": f"load failed: {e}"}

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _nms(self, boxes: List[List[int]], confs: List[float]) -> List[int]:
        if not boxes: return []
        idxs = cv2.dnn.NMSBoxes(boxes, confs, OBJ_CONF_THRESH, OBJ_NMS_THRESH)
        if idxs is None or len(idxs) == 0: return []
        if isinstance(idxs, np.ndarray): return [int(i) for i in idxs.flatten().tolist()]
        return [int(i) for i in idxs]

    def _infer_once(self, frame: np.ndarray, swap_rb: bool, conf_th: float) -> Tuple[List[Dict[str, Any]], float]:
        (h, w) = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, DNN_IN_SIZE),
                                     DNN_SCALE, DNN_IN_SIZE, DNN_MEAN, swapRB=swap_rb, crop=False)
        self.net.setInput(blob)
        detections = self.net.forward()
        out_objects: List[Dict[str, Any]] = []
        boxes_all: List[List[int]] = []
        confs_all: List[float] = []
        labels_all: List[str] = []
        conf_max = 0.0

        for i in range(detections.shape[2]):
            conf = float(detections[0, 0, i, 2])
            conf_max = max(conf_max, conf)
            if conf < conf_th: continue
            idx = int(detections[0, 0, i, 1])
            if idx < 0 or idx >= len(OBJ_CLASSES): continue
            label = OBJ_CLASSES[idx]
            if label != "person" and label not in ANIMAL_CLASSES: continue
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            x1, y1, x2, y2 = box.astype("int")
            x, y = max(0, x1), max(0, y1)
            rw, rh = max(0, x2 - x), max(0, y2 - y)
            if rw * rh <= 0: continue
            boxes_all.append([int(x), int(y), int(rw), int(rh)])
            confs_all.append(conf)
            labels_all.append(label)

        keep = self._nms(boxes_all, confs_all)
        for k in keep:
            out_objects.append({"label": labels_all[k], "conf": float(confs_all[k]), "box": [int(v) for v in boxes_all[k]]})
        out_objects.sort(key=lambda o: o["conf"], reverse=True)
        out_objects = out_objects[:15]
        return out_objects, conf_max

    def _run(self):
        min_interval = 1.0 / OBJECTS_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue
            frame = cv2.imdecode(np.frombuffer(jpeg, dtype=np.uint8), cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            out_objects: List[Dict[str, Any]] = []
            conf_max = 0.0

            if self.ok and self.net is not None:
                try:
                    out_objects, conf_max = self._infer_once(frame, self.swap_rb, OBJ_CONF_THRESH)
                except Exception as e:
                    self.ok = False
                    self._last = {"ok": False, "backend": self.backend, "fps_obj": round(self._fps,2),
                                  "objects": [], "ts": int(time.time()*1000), "error": f"forward failed: {e}"}
                    time.sleep(0.1)
                    continue

                # Fallback: se não teve nada por um tempo, tente inverter canais e mantenha se melhorar
                if conf_max < 0.05:
                    self._nohit_streak += 1
                else:
                    self._nohit_streak = 0

                if self._nohit_streak >= 10:  # ~1 segundo
                    try_other, conf_other = self._infer_once(frame, not self.swap_rb, max(0.1, OBJ_CONF_THRESH*0.8))
                    if conf_other > conf_max + 0.05:
                        self.swap_rb = not self.swap_rb   # trava no melhor
                        out_objects, conf_max = try_other, conf_other
                    self._nohit_streak = 0

            with self._lock:
                self._last_conf_max = conf_max
                self._last = {
                    "ok": bool(self.ok),
                    "backend": self.backend,
                    "fps_obj": round(self._fps, 2),
                    "objects": out_objects,
                    "ts": int(time.time()*1000),
                    "proto": self.proto_path,
                    "weights": self.weights_path,
                    "conf_max": round(conf_max, 3),
                    "swap_rb": self.swap_rb,
                }

            self._frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._fps = self._frames / (now - self._last_fps_tick)
                self._frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(max(0.0, min_interval - elapsed))

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._last)

objects_det = ObjectsDetector(grabber)
objects_det.start()

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "balanced_detector",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    res = detector.get_result()
    res["objects"] = objects_det.get()
    if res.get("ts", 0) and (int(time.time() * 1000) - res["ts"] <= MAX_RESULT_AGE_MS):
        return res
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    mask_hsv = hsv_fire_mask(frame)
    ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
    v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
    score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)
    is_fire = bool(score_raw >= HYST_HIGH)
    res2 = {
        "ok": True, "isFire": is_fire,
        "score": round(float(score_raw), 3),
        "score_raw": round(float(score_raw), 3),
        "score_ema": round(float(score_raw), 3),
        "boxes": [], "ts": int(time.time() * 1000), "fallback": True,
        "objects": objects_det.get()
    }
    return res2

@app.get("/objects")
def objects(conf: float = Query(None, description="override confidence threshold (0..1)")):
    global OBJ_CONF_THRESH
    if conf is not None:
        OBJ_CONF_THRESH = max(0.05, min(0.95, float(conf)))
    return objects_det.get()

@app.get("/objects_raw")
def objects_raw(conf: float = Query(0.2, description="threshold (0..1)")):
    """Lista TUDO que o modelo vê (sem filtrar só pessoa/animais) para debug."""
    o = objects_det.get()
    return {
        "ok": o.get("ok", False),
        "backend": o.get("backend"),
        "proto": o.get("proto"),
        "weights": o.get("weights"),
        "conf_max": o.get("conf_max"),
        "swap_rb": o.get("swap_rb"),
        "note": "este endpoint só ecoa status; para ver deteções brutas use /objects_frame",
    }

@app.get("/objects_frame")
def objects_frame(conf: float = Query(0.2, description="threshold (0..1)")):
    """Retorna um JPEG com caixas desenhadas (pessoa/animais) — debug visual."""
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return Response(content=placeholder_jpeg("NO LIVE FRAME"), media_type="image/jpeg")
    frame = cv2.imdecode(np.frombuffer(jpeg, dtype=np.uint8), cv2.IMREAD_COLOR)
    if frame is None:
        return Response(content=placeholder_jpeg("DECODE ERR"), media_type="image/jpeg")

    # roda uma inferência direta (mesmo pipeline do detector)
    det = objects_det
    if not det.ok or det.net is None:
        return Response(content=placeholder_jpeg("NET NOT LOADED"), media_type="image/jpeg")

    objs, _ = det._infer_once(frame, det.swap_rb, max(0.05, min(0.95, conf)))
    for o in objs:
        x,y,w,h = o["box"]
        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)
        cv2.putText(frame, f'{o["label"]} {o["conf"]:.2f}', (x, max(15,y-6)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", frame, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])
    return Response(content=buf.tobytes(), media_type="image/jpeg")

@app.get("/debug_models")
def debug_models():
    o = objects_det.get()
    s = grabber.status()
    return {
        "ok": True,
        "fire_model": "heuristic-balanced",
        "objects_backend": o.get("backend", "mobilenet-ssd"),
        "objects_ok": o.get("ok", False),
        "fps_obj": o.get("fps_obj", 0.0),
        "num_objects": len(o.get("objects", [])),
        "proto": o.get("proto"),
        "weights": o.get("weights"),
        "conf_max": o.get("conf_max"),
        "swap_rb": o.get("swap_rb"),
        "conf_thresh": OBJ_CONF_THRESH,
        "nms_thresh": OBJ_NMS_THRESH,
        "stream_hasFrame": s.get("hasFrame"),
        "stream_fps_in": s.get("fps_in"),
        "error": o.get("error"),
    }




Passo a passo (rápido)




Reinicie:




uvicorn server_heuristic:app --host 0.0.0.0 --port 8000 --reload





Abra no navegador do PC do servidor:






http://SEU_PC:8000/debug_models

Verifique:



objects_ok: true


proto e weights apontando para os arquivos dentro de models/


stream_hasFrame: true e stream_fps_in > 0


swap_rb (começa false; o servidor muda sozinho se perceber que não tem hits)






http://SEU_PC:8000/objects

Observe conf_max. Se ficar ~0.0, aguarde 2–3s (o fallback tenta swapRB=True sozinho).


http://SEU_PC:8000/objects?conf=0.15

Abaixe o limiar temporariamente (se luz ruim).


http://SEU_PC:8000/objects_frame?conf=0.15

Deve aparecer um JPEG com as caixas em pessoas/animais. Se não aparecer nada, ainda é par de arquivos.






No app (tela Câmera):






Servidor = http://SEU_PC:8000


Clique Detectar. Os badges devem sair de 0 quando alguém entrar no quadro.





Por que isso resolve?




Se o seu caffemodel exigir RGB (incomum em Caffe, mas acontece em forks), o servidor agora detecta e alterna automaticamente (swap_rb muda para true sozinho após ~1s sem hits).


Se o seu par deploy/weights não estiver combinando, o /debug_models mostra quais arquivos e o /objects_frame deixa claro se a saída está vazia por threshold ou por modelo.


Você ainda pode ajustar o threshold via query sem reiniciar.




Se ainda assim o /objects_frame não desenhar nada, me diga o que aparece em /debug_models (proto, weights, conf_max, swap_rb, stream_hasFrame) que eu ajusto pontualmente.







































































Beleza — se o /objects veio vazio mesmo com uma pessoa no quadro, o problema está no servidor (não no app). Provavelmente é par de arquivos não correspondentes (prototxt vs caffemodel) ou limiar de confiança alto para a sua cena/iluminação.


Abaixo te mando um server_heuristic.py atualizado (COMPLETO) que:




aceita vários nomes de prototxt/weights (inclui deploy.prototxt e mobilenet_iter_73000.caffemodel que apareceram no seu clone);


mostra no /debug_models exatamente quais arquivos foram carregados e qual foi a maior confiança da última inferência;


reduz o threshold padrão para 0.25 (mais sensível) — você pode ajustar por query string (/objects?conf=0.35);


mantém todo o detector de fogo inalterado.




Depois do código, tem um passo-a-passo de teste para você validar rapidinho.



server_heuristic.py (substitua o arquivo inteiro)


# server_heuristic.py
# FOGO (heurístico, inalterado) + OBJETOS (MobileNet-SSD Caffe)
import time
import threading
from collections import deque
from typing import Optional, List, Tuple, Dict, Any

import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os

# ===================== CONFIG =====================
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
BOUNDARY = b"--frame"
CONNECT_TIMEOUT = 3.0
READ_TIMEOUT = 3.0

JPEG_QUALITY = 85

# HSV mais amplo p/ fogo real (amarelo/laranja)
HSV_LOW = (8, 80, 120)
HSV_HIGH = (40, 255, 255)

# Dominância de vermelho (reforço, não bloqueio)
RED_DELTA = 15

# Detector fogo
DETECTOR_MAX_FPS = 14.0
HYST_HIGH = 0.18
HYST_LOW  = 0.15
VOTE_WINDOW = 7
VOTE_NEED   = 4
EMA_ALPHA   = 0.25
MIN_BLOB_AREA = 1200
KERNEL_SZ = 5

# Anti-movimento
MOTION_THRESH = 22
MOTION_DILATE_ITERS = 1

# Persistência espacial
PERSIST_CONSEC = 2
IOU_MIN = 0.15

# Idades máximas
MAX_FRAME_AGE_MS = 3000
MAX_RESULT_AGE_MS = 800

# ===================== OBJETOS (MobileNet-SSD) =====================
OBJECTS_MAX_FPS   = 10.0
OBJ_CONF_THRESH   = 0.25     # <= mais sensível; pode subir via /objects?conf=0.4
OBJ_NMS_THRESH    = 0.45
OBJ_CLASSES       = [
    "background","aeroplane","bicycle","bird","boat","bottle","bus","car","cat","chair",
    "cow","diningtable","dog","horse","motorbike","person","pottedplant","sheep","sofa","train","tvmonitor"
]
ANIMAL_CLASSES    = {"bird","cat","cow","dog","horse","sheep"}
DNN_IN_SIZE       = (300, 300)
DNN_SCALE         = 0.007843
DNN_MEAN          = 127.5

# Candidatos de caminhos — cobrimos os nomes que você tem
DNN_PROTOTXT_CANDIDATES = [
    "./models/MobileNetSSD_deploy.prototxt",
    "./models/MobileNetSSD_deploy.prototxt.txt",
    "./models/deploy.prototxt",
    "./models/voc/MobileNetSSD_test.prototxt",     # só se você colocou assim
]
DNN_CAFFE_WEIGHTS_CANDIDATES = [
    "./models/MobileNetSSD_deploy.caffemodel",
    "./models/mobilenet_iter_73000.caffemodel",
]

# ===================== FASTAPI =====================
app = FastAPI(title="HydroBot Fire + Objects (MobileNetSSD)", version="1.3.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ===================== PLACEHOLDER =====================
def placeholder_jpeg(msg: str = "NO FRAME") -> bytes:
    img = np.zeros((270, 480, 3), dtype=np.uint8)
    img[:, :] = (40, 40, 200)
    cv2.putText(img, msg, (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 2, cv2.LINE_AA)
    cv2.putText(img, time.strftime("%H:%M:%S"), (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2, cv2.LINE_AA)
    ok, buf = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), 80])
    return buf.tobytes()

# ===================== GRABBER CONTÍNUO =====================
class MJPEGGrabber:
    def __init__(self):
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._ip = CAMERA_IP
        self._last_jpeg: Optional[bytes] = None
        self._last_ts_ms: int = 0
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

    def start(self, ip: Optional[str] = None):
        if ip:
            self._ip = ip
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        while not self._stop.is_set():
            url = STREAM_URL_FMT.format(self._ip)
            try:
                with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
                    if r.status_code != 200:
                        time.sleep(0.5); continue
                    buf = b""
                    MAX_BYTES = 4_000_000
                    self._frames = 0
                    self._last_fps_tick = time.time()
                    for chunk in r.iter_content(chunk_size=4096):
                        if self._stop.is_set(): break
                        if not chunk: continue
                        buf += chunk
                        if len(buf) > MAX_BYTES: buf = b""
                        i = buf.find(BOUNDARY)
                        if i == -1: continue
                        hdr_start = i + len(BOUNDARY)
                        while hdr_start + 2 <= len(buf) and buf[hdr_start:hdr_start+2] == b"\r\n":
                            hdr_start += 2
                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1: continue
                        headers_bytes = buf[hdr_start:headers_end]
                        content_length = None
                        for line in headers_bytes.split(b"\r\n"):
                            if line.lower().startswith(b"content-length:"):
                                try: content_length = int(line.split(b":", 1)[1].strip())
                                except: pass
                                break
                        img_start = headers_end + 4
                        jpeg_bytes = None
                        if content_length is not None:
                            if len(buf) < img_start + content_length: continue
                            jpeg_bytes = buf[img_start:img_start + content_length]
                            buf = buf[img_start + content_length:]
                        else:
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                buf = buf[j:]
                            else:
                                continue
                        if jpeg_bytes:
                            ts_ms = int(time.time() * 1000)
                            with self._lock:
                                self._last_jpeg = jpeg_bytes
                                self._last_ts_ms = ts_ms
                            self._frames += 1
                            now = time.time()
                            if now - self._last_fps_tick >= 1.0:
                                self._fps = self._frames / (now - self._last_fps_tick)
                                self._frames = 0
                                self._last_fps_tick = now
            except requests.exceptions.RequestException:
                time.sleep(0.5)
            except Exception:
                time.sleep(0.5)

    def get_latest_jpeg(self, max_age_ms: int = MAX_FRAME_AGE_MS) -> Optional[bytes]:
        with self._lock:
            if self._last_jpeg is None: return None
            if int(time.time() * 1000) - self._last_ts_ms > max_age_ms: return None
            return self._last_jpeg

    def status(self):
        with self._lock:
            age_ms = (int(time.time() * 1000) - self._last_ts_ms) if self._last_ts_ms else None
            return {"ip": self._ip, "hasFrame": self._last_jpeg is not None, "age_ms": age_ms,
                    "fps_in": round(self._fps, 2), "ts_ms": self._last_ts_ms}

grabber = MJPEGGrabber()
grabber.start(CAMERA_IP)

# ===================== UTILs VISÃO =====================
def rgb_red_dominance_mask(frame_bgr: np.ndarray, delta: int = RED_DELTA) -> np.ndarray:
    b, g, r = cv2.split(frame_bgr)
    mask = (r.astype(np.int16) > (g.astype(np.int16) + delta)) & (r.astype(np.int16) > (b.astype(np.int16) + delta))
    return (mask.astype(np.uint8)) * 255

def hsv_fire_mask(frame_bgr: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2HSV)
    lower = np.array(HSV_LOW, dtype=np.uint8)
    upper = np.array(HSV_HIGH, dtype=np.uint8)
    return cv2.inRange(hsv, lower, upper)

def skin_mask_ycrcb(frame_bgr: np.ndarray) -> np.ndarray:
    ycrcb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    skin = cv2.inRange(ycrcb, (0, 133, 77), (255, 173, 127))
    dark = cv2.threshold(y, 60, 255, cv2.THRESH_BINARY)[1]
    skin = cv2.bitwise_and(skin, dark)
    return skin

def iou(a: Tuple[int,int,int,int], b: Tuple[int,int,int,int]) -> float:
    ax, ay, aw, ah = a; bx, by, bw, bh = b
    ax2, ay2 = ax + aw, ay + ah; bx2, by2 = bx + bw, by + bh
    ix1, iy1 = max(ax, bx), max(ay, by)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0, ix2 - ix1), max(0, iy2 - iy1)
    inter = iw * ih; union = aw*ah + bw*bh - inter
    return float(inter) / float(union) if union > 0 else 0.0

def boxes_from_mask(mask_bin: np.ndarray, min_area: int = MIN_BLOB_AREA) -> List[List[int]]:
    k = np.ones((KERNEL_SZ, KERNEL_SZ), np.uint8)
    m = cv2.morphologyEx(mask_bin, cv2.MORPH_OPEN, k, 1)
    m = cv2.morphologyEx(m, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        if w * h >= min_area:
            boxes.append([x, y, w, h])
    return boxes

# ===================== DETECTOR CONTÍNUO (FOGO) =====================
class Detector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self._prev_gray: Optional[np.ndarray] = None
        self._score_raw = 0.0
        self._score_ema = 0.0
        self._is_fire = False
        self._boxes: List[List[int]] = []
        self._votes = deque(maxlen=VOTE_WINDOW)
        self._persist_hits = 0
        self._last_main_box: Optional[Tuple[int,int,int,int]] = None

        self._det_fps = 0.0
        self._det_frames = 0
        self._last_fps_tick = time.time()
        self._last_result_ts = 0

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _run(self):
        min_interval = 1.0 / DETECTOR_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue

            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            # --- máscaras base ---
            mask_hsv = hsv_fire_mask(frame)
            mask_skin = skin_mask_ycrcb(frame)
            mask_red  = rgb_red_dominance_mask(frame)

            # anti-movimento
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.GaussianBlur(gray, (3, 3), 0)
            motion_mask = np.zeros_like(gray, dtype=np.uint8)
            if self._prev_gray is not None:
                diff = cv2.absdiff(gray, self._prev_gray)
                _, motion_mask = cv2.threshold(diff, MOTION_THRESH, 255, cv2.THRESH_BINARY)
                if MOTION_DILATE_ITERS > 0:
                    k = np.ones((3, 3), np.uint8)
                    motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_DILATE, k, MOTION_DILATE_ITERS)
            self._prev_gray = gray

            # combinação principal: HSV ∧ ¬pele ∧ ¬movimento
            stable = cv2.bitwise_and(mask_hsv, cv2.bitwise_not(mask_skin))
            stable = cv2.bitwise_and(stable, cv2.bitwise_not(motion_mask))

            # reforço
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            V = hsv[..., 2]
            bright = cv2.threshold(V, 200, 255, cv2.THRESH_BINARY)[1]
            red_boost = cv2.bitwise_and(mask_red, bright)
            combined = cv2.bitwise_or(stable, red_boost)

            # scores
            ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
            v_mean = float(np.mean(V)) / 255.0
            score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)

            ratio_combined = float(np.count_nonzero(combined)) / float(combined.size)
            score_combined = min(1.0, ratio_combined * 5.0 + v_mean * 0.1)

            ema = score_combined if self._score_ema == 0.0 else (EMA_ALPHA * score_combined + (1.0 - EMA_ALPHA) * self._score_ema)

            # caixas
            boxes = boxes_from_mask(combined, MIN_BLOB_AREA)

            # persistência espacial leve
            main_box = None
            if boxes:
                areas = [w*h for (_,_,w,h) in boxes]
                main_box = boxes[int(np.argmax(areas))]
                if self._last_main_box is not None:
                    if iou(tuple(main_box), tuple(self._last_main_box)) >= IOU_MIN:
                        self._persist_hits += 1
                    else:
                        self._persist_hits = 1
                else:
                    self._persist_hits = 1
            else:
                self._persist_hits = 0
            self._last_main_box = tuple(main_box) if main_box is not None else None

            # histerese + votos
            if ema >= HYST_HIGH and self._persist_hits >= PERSIST_CONSEC:
                guess = 1
            elif ema <= HYST_LOW:
                guess = 0
            else:
                guess = (1 if (len(self._votes) > 0 and self._votes[-1] == 1 and self._persist_hits >= PERSIST_CONSEC) else 0)

            self._votes.append(guess)
            final_fire = 1 if sum(self._votes) >= VOTE_NEED else 0

            with self._lock:
                self._score_raw = float(score_raw)
                self._score_ema = float(ema)
                self._is_fire = bool(final_fire == 1)
                self._boxes = boxes if self._is_fire else []
                self._last_result_ts = int(time.time() * 1000)

            # FPS detector
            self._det_frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._det_fps = self._det_frames / (now - self._last_fps_tick)
                self._det_frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(min_interval - elapsed)

    def get_result(self) -> Dict[str, Any]:
        with self._lock:
            return {
                "ok": True,
                "isFire": self._is_fire,
                "score": round(self._score_ema, 3),
                "score_raw": round(self._score_raw, 3),
                "score_ema": round(self._score_ema, 3),
                "boxes": self._boxes,
                "ts": self._last_result_ts,
                "fps_det": round(self._det_fps, 2),
                "vote": {"win": VOTE_WINDOW, "need": VOTE_NEED, "sum": int(sum(self._votes))},
                "persist": {"hits": self._persist_hits, "need": PERSIST_CONSEC, "iou_min": IOU_MIN},
                "hyst": {"hi": HYST_HIGH, "lo": HYST_LOW},
            }

detector = Detector(grabber)
detector.start()

# ===================== DETECTOR DE OBJETOS =====================
class ObjectsDetector:
    def __init__(self, src: MJPEGGrabber):
        self.src = src
        self._lock = threading.Lock()
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

        self.backend = "mobilenet-ssd"
        self.net = None
        self.ok = False
        self.proto_path = None
        self.weights_path = None

        # métricas p/ debug
        self._last_conf_max = 0.0

        # FPS
        self._frames = 0
        self._fps = 0.0
        self._last_fps_tick = time.time()

        # último resultado
        self._last: Dict[str, Any] = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0}

        self._load_net()

    def _load_net(self):
        # acha prototxt
        for p in DNN_PROTOTXT_CANDIDATES:
            if os.path.exists(p):
                self.proto_path = p
                break
        # acha weights
        for w in DNN_CAFFE_WEIGHTS_CANDIDATES:
            if os.path.exists(w):
                self.weights_path = w
                break

        if not self.proto_path:
            self.ok = False
            self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0,
                          "error": "prototxt not found"}
            return
        if not self.weights_path:
            self.ok = False
            self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0,
                          "error": "caffemodel not found"}
            return
        try:
            self.net = cv2.dnn.readNetFromCaffe(self.proto_path, self.weights_path)
            # força CPU (compatível)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            self.ok = True
        except Exception as e:
            self.ok = False
            self._last = {"ok": False, "backend": self.backend, "fps_obj": 0.0, "objects": [], "ts": 0,
                          "error": f"load failed: {e}"}

    def start(self):
        self.stop()
        self._stop.clear()
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)
        self._thread = None

    def _nms(self, boxes: List[List[int]], confs: List[float]) -> List[int]:
        if not boxes: return []
        idxs = cv2.dnn.NMSBoxes(boxes, confs, OBJ_CONF_THRESH, OBJ_NMS_THRESH)
        if idxs is None or len(idxs) == 0: return []
        if isinstance(idxs, np.ndarray):
            return [int(i) for i in idxs.flatten().tolist()]
        return [int(i) for i in idxs]

    def _run(self):
        min_interval = 1.0 / OBJECTS_MAX_FPS
        while not self._stop.is_set():
            t0 = time.time()
            jpeg = self.src.get_latest_jpeg()
            if jpeg is None:
                time.sleep(0.01); continue
            arr = np.frombuffer(jpeg, dtype=np.uint8)
            frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
            if frame is None:
                time.sleep(0.005); continue

            out_objects: List[Dict[str, Any]] = []
            conf_max = 0.0

            if self.ok and self.net is not None:
                (h, w) = frame.shape[:2]
                blob = cv2.dnn.blobFromImage(cv2.resize(frame, DNN_IN_SIZE),
                                             DNN_SCALE, DNN_IN_SIZE, DNN_MEAN, swapRB=False, crop=False)
                try:
                    self.net.setInput(blob)
                    detections = self.net.forward()
                except Exception as e:
                    self.ok = False
                    self._last = {"ok": False, "backend": self.backend, "fps_obj": round(self._fps,2),
                                  "objects": [], "ts": int(time.time()*1000), "error": f"forward failed: {e}"}
                    time.sleep(0.1)
                    continue

                boxes_all: List[List[int]] = []
                confs_all: List[float] = []
                labels_all: List[str] = []

                # detections: [1,1,N,7] => [img_id, class_id, conf, x1,y1,x2,y2]
                for i in range(detections.shape[2]):
                    conf = float(detections[0, 0, i, 2])
                    conf_max = max(conf_max, conf)
                    if conf < OBJ_CONF_THRESH:
                        continue
                    idx = int(detections[0, 0, i, 1])
                    if idx < 0 or idx >= len(OBJ_CLASSES):
                        continue
                    label = OBJ_CLASSES[idx]
                    if label != "person" and label not in ANIMAL_CLASSES:
                        continue
                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                    x1, y1, x2, y2 = box.astype("int")
                    x, y = max(0, x1), max(0, y1)
                    rw, rh = max(0, x2 - x), max(0, y2 - y)
                    if rw * rh <= 0: continue
                    boxes_all.append([int(x), int(y), int(rw), int(rh)])
                    confs_all.append(conf)
                    labels_all.append(label)

                keep = self._nms(boxes_all, confs_all)
                for k in keep:
                    out_objects.append({
                        "label": labels_all[k],
                        "conf": float(confs_all[k]),
                        "box": [int(v) for v in boxes_all[k]]
                    })

                out_objects.sort(key=lambda o: o["conf"], reverse=True)
                out_objects = out_objects[:15]

            with self._lock:
                self._last_conf_max = conf_max
                self._last = {
                    "ok": bool(self.ok),
                    "backend": self.backend,
                    "fps_obj": round(self._fps, 2),
                    "objects": out_objects,
                    "ts": int(time.time()*1000),
                    "proto": self.proto_path,
                    "weights": self.weights_path,
                    "conf_max": round(conf_max, 3),
                }

            self._frames += 1
            now = time.time()
            if now - self._last_fps_tick >= 1.0:
                self._fps = self._frames / (now - self._last_fps_tick)
                self._frames = 0
                self._last_fps_tick = now

            elapsed = time.time() - t0
            if elapsed < min_interval:
                time.sleep(max(0.0, min_interval - elapsed))

    def get(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._last)

objects_det = ObjectsDetector(grabber)
objects_det.start()

# ===================== ENDPOINTS =====================
@app.get("/status")
def status():
    s1 = grabber.status()
    s2 = detector.get_result()
    return {"ok": True, "camera_ip": s1["ip"], "model": "balanced_detector",
            "fps_in": s1["fps_in"], "hasFrame": s1["hasFrame"], "age_ms": s1["age_ms"], **s2}

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    grabber.start(CAMERA_IP)
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        jpeg = placeholder_jpeg("NO LIVE FRAME")
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    res = detector.get_result()
    if res.get("ts", 0) and (int(time.time() * 1000) - res["ts"] <= MAX_RESULT_AGE_MS):
        res["objects"] = objects_det.get()
        return res
    jpeg = grabber.get_latest_jpeg(max_age_ms=MAX_FRAME_AGE_MS)
    if jpeg is None:
        return {"ok": False, "error": "no recent frame"}
    arr = np.frombuffer(jpeg, dtype=np.uint8)
    frame = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    mask_hsv = hsv_fire_mask(frame)
    ratio_hsv = float(np.count_nonzero(mask_hsv)) / float(mask_hsv.size)
    v_mean = float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)[..., 2])) / 255.0
    score_raw = min(1.0, ratio_hsv * 4.0 + v_mean * 0.2)
    is_fire = bool(score_raw >= HYST_HIGH)
    return {
        "ok": True, "isFire": is_fire,
        "score": round(float(score_raw), 3),
        "score_raw": round(float(score_raw), 3),
        "score_ema": round(float(score_raw), 3),
        "boxes": [], "ts": int(time.time() * 1000), "fallback": True,
        "objects": objects_det.get()
    }

@app.get("/objects")
def objects(conf: float = Query(None, description="override confidence threshold (0..1)")):
    global OBJ_CONF_THRESH
    if conf is not None:
        # permite ajustar on-the-fly sem reiniciar
        OBJ_CONF_THRESH = max(0.05, min(0.95, float(conf)))
    return objects_det.get()

@app.get("/debug_models")
def debug_models():
    o = objects_det.get()
    return {
        "ok": True,
        "fire_model": "heuristic-balanced",
        "objects_backend": o.get("backend", "mobilenet-ssd"),
        "objects_ok": o.get("ok", False),
        "fps_obj": o.get("fps_obj", 0.0),
        "num_objects": len(o.get("objects", [])),
        "proto": o.get("proto"),
        "weights": o.get("weights"),
        "conf_max": o.get("conf_max"),
        "error": o.get("error"),
        "conf_thresh": OBJ_CONF_THRESH,
        "nms_thresh": OBJ_NMS_THRESH,
    }




Teste rápido (para achar a causa)




Reinicie:




uvicorn server_heuristic:app --host 0.0.0.0 --port 8000 --reload





Abra no navegador (no PC do servidor):






http://SEU_PC:8000/debug_models

Confira:



objects_ok: true


proto aponta para um dos candidatos (ex.: ./models/deploy.prototxt ou ...MobileNetSSD_deploy.prototxt)


weights aponta para mobilenet_iter_73000.caffemodel ou MobileNetSSD_deploy.caffemodel






http://SEU_PC:8000/objects

Fique na frente da câmera por 2–3s e veja conf_max subir (ex.: 0.6, 0.8), e aparecer items em objects.






Se ainda vier vazio:






Tente baixar o limiar no voo:

http://SEU_PC:8000/objects?conf=0.15


Recarregue /objects.

Se então aparecer, ficou claro que era threshold alto para a sua cena.

Depois fixe o valor que funcionar melhor.






Se objects_ok: false ou error: load failed:






É par de arquivos incompatível. Use deploy.prototxt do mesmo repo do mobilenet_iter_73000.caffemodel (o que você clonou).


Atualize OpenCV: pip install --upgrade opencv-python.





No app (Câmera)


Com o servidor ok (objects preenchido), a tela que te passei já mostra:




🔥 fogo (como antes)


👤 Pessoas: N


🐾 Animais: M




Se /objects mostrar itens mas o app não contar, me manda um pequeno JSON retornado por /detect (só as primeiras linhas) para eu ajustar o caminho exato — mas já deixei j.objects.objects (estrutura que sai do servidor acima).


Vamos ver o que aparece no /debug_models (principalmente proto, weights, conf_max) e se baixar conf faz os objetos surgirem.




























































Vamos direto ao ponto: o servidor está ok, mas a tela Câmera não estava exibindo o feedback de pessoas/animais que o endpoint já envia. Abaixo te passo:




como testar o servidor rapidinho (para confirmar que ele está detectando),


o arquivo completo CameraScreen.tsx com as mudanças para mostrar contagem estável de pessoas e animais quando você clica em Detectar (sem depender de novas libs nem desenhar caixas).





1) Teste rápido do servidor (confirma se está detectando)


No PC do servidor, com a câmera apontando para alguém/animal:






Abra http://SEU_PC:8000/debug_models

→ Deve ter: "objects_backend":"mobilenet-ssd", "objects_ok": true.






Abra http://SEU_PC:8000/objects

→ Deve aparecer algo assim quando há alguém no quadro:


{
  "ok": true,
  "backend": "mobilenet-ssd",
  "objects": [
    {"label":"person","conf":0.82,"box":[...]}
  ]
}







Se /objects trouxer lista vazia mesmo com pessoa no quadro, me diga que ajusto os thresholds; mas normalmente aparece.



2) Substitua sua tela por ESTE CameraScreen.tsx (COMPLETO)


Coloque em:

HydroBotApp2/src/screens/CameraScreen.tsx




O que muda: enquanto Detectar estiver ativo, o app consulta /detect ~5 Hz e mostra dois badges com contagem de 👤 Pessoas e 🐾 Animais. Não precisa de bibliotecas novas. O vídeo continua com STREAM (rápido) ≥20 fps ou SNAPSHOT (fallback), como você já tinha.




// HydrobotApp2/src/screens/CameraScreen.tsx
import { useEffect, useMemo, useRef, useState } from "react";
import {
  View, Text, TextInput, Pressable, StyleSheet, PanResponder,
  GestureResponderEvent, PanResponderGestureState, Image, Animated,
} from "react-native";
import { WebView } from "react-native-webview";
import { saveEventFromServer } from "../storage/fireLog";
import { useLanguage } from "../context/LanguageContext";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

/* ============================
   A) VISUAL 1 — STREAM (≥20fps)
   ============================ */
function LiveMJPEG({ ip }: { ip: string }) {
  const html = `
    <html><head><meta name="viewport" content="width=device-width, initial-scale=1">
      <style>
        html,body{margin:0;padding:0;background:#000;height:100%;}
        .wrap{position:fixed;inset:0;display:flex;align-items:center;justify-content:center;background:#000;}
        img{max-width:100%;max-height:100%;object-fit:contain;background:#000;}
      </style>
    </head>
    <body>
      <div class="wrap">
        <img src="http://${ip}:81/stream" />
      </div>
    </body></html>
  `;
  return (
    <WebView
      originWhitelist={["*"]}
      source={{ html }}
      javaScriptEnabled
      allowsInlineMediaPlayback
      mediaPlaybackRequiresUserAction={false}
      automaticallyAdjustContentInsets={false}
      style={{ flex: 1, backgroundColor: "black" }}
    />
  );
}

/* ==========================================
   B) VISUAL 2 — SNAPSHOT com crossfade (fallback)
   ========================================== */
function CrossfadeImage({
  currentUri,
  nextUri,
  onNextLoadStart,
  onNextShown,
}: {
  currentUri: string;
  nextUri: string;
  onNextLoadStart: () => void;
  onNextShown: () => void;
}) {
  const fade = useRef(new Animated.Value(0)).current;
  const [showNext, setShowNext] = useState(false);
  const lastNextRef = useRef<string>("");

  useEffect(() => {
    if (!nextUri || nextUri === lastNextRef.current) return;
    lastNextRef.current = nextUri;
    setShowNext(true);
  }, [nextUri]);

  return (
    <View style={{ flex: 1, backgroundColor: "black" }}>
      {currentUri ? (
        <Image source={{ uri: currentUri }} style={{ flex: 1, width: "100%" }} resizeMode="contain" />
      ) : null}

      {showNext ? (
        <Animated.View style={[StyleSheet.absoluteFill, { opacity: fade }]}>
          <Image
            source={{ uri: nextUri }}
            style={{ flex: 1, width: "100%" }}
            resizeMode="contain"
            onLoadStart={onNextLoadStart}
            onError={() => { onNextShown(); }}
            onLoadEnd={() => {
              Animated.timing(fade, { toValue: 1, duration: 80, useNativeDriver: true }).start(() => {
                onNextShown();
                fade.setValue(0);
                setShowNext(false);
              });
            }}
          />
        </Animated.View>
      ) : null}
    </View>
  );
}

/* ============================
   TEXTOS
   ============================ */
const textsByLang = {
  pt: {
    espIp: "ESP IP:", test: "Testar", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "BOMBA ON", pumpOff: "BOMBA OFF", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Salvando...",
    statusFail: "Falha ao conectar. Confira o Wi-Fi e IP.",
    noVideo: "Sem vídeo (snapshot). Verifique o servidor.",
    fireOn: (s:number)=>`🔥 FOGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sem fogo • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento salvo em ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    view: "Visão:",
    stream: "STREAM (rápido)",
    snapshot: "SNAPSHOT (fallback)",
    people: (n:number)=>`👤 Pessoas: ${n}`,
    animals: (n:number)=>`🐾 Animais: ${n}`,
  },
  en: {
    espIp: "ESP IP:", test: "Test", ledOn: "LED ON", ledOff: "LED OFF",
    pumpOn: "PUMP ON", pumpOff: "PUMP OFF", server: "Server:",
    detect: "Detect", detecting: "Detecting", saving: "Saving...",
    statusFail: "Failed to connect. Check Wi-Fi/IP.",
    noVideo: "No video (snapshot). Check the server.",
    fireOn: (s:number)=>`🔥 FIRE • score ${s.toFixed(2)}`, fireOff: (s:number)=>`No fire • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Event saved at ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    view: "View:",
    stream: "STREAM (fast)",
    snapshot: "SNAPSHOT (fallback)",
    people: (n:number)=>`👤 People: ${n}`,
    animals: (n:number)=>`🐾 Animals: ${n}`,
  },
  es: {
    espIp: "ESP IP:", test: "Probar", ledOn: "LED ENC.", ledOff: "LED APAG.",
    pumpOn: "BOMBA ENC.", pumpOff: "BOMBA APAG.", server: "Servidor:",
    detect: "Detectar", detecting: "Detectando", saving: "Guardando...",
    statusFail: "Error de conexión. Revisa Wi-Fi/IP.",
    noVideo: "Sin vídeo (snapshot). Revisa el servidor.",
    fireOn: (s:number)=>`🔥 FUEGO • score ${s.toFixed(2)}`, fireOff: (s:number)=>`Sin fuego • score ${s.toFixed(2)}`,
    eventSavedAt: (ts:string)=>`Evento guardado a las ${ts}`,
    placeholderIp: "192.168.4.1",
    placeholderServer: "http://192.168.4.2:8000",
    view: "Vista:",
    stream: "STREAM (rápido)",
    snapshot: "SNAPSHOT (fallback)",
    people: (n:number)=>`👤 Personas: ${n}`,
    animals: (n:number)=>`🐾 Animales: ${n}`,
  },
};

const DEFAULT_IP = "192.168.4.1";
const DEFAULT_SERVER = "http://192.168.4.2:8000";

type ObjDet = { label: string; conf: number; box: [number, number, number, number] };

export default function CameraScreen() {
  const { lang } = useLanguage();
  const T = textsByLang[lang];

  // conexões
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState("—");
  const [ledOn, setLedOn] = useState(false);
  const [pumpOn, setPumpOn] = useState(false);

  // detecção
  const [detectOn, setDetectOn] = useState(false);
  const [isFire, setIsFire] = useState(false);
  const [fireScore, setFireScore] = useState(0);

  // pessoas/animais (feedback visual)
  const [peopleCount, setPeopleCount] = useState(0);
  const [animalCount, setAnimalCount] = useState(0);

  // registro automático
  const [saving, setSaving] = useState(false);
  const lastSaveRef = useRef(0);
  const wasFireRef = useRef(false);
  const SAVE_COOLDOWN_MS = 5000;

  // modo de visualização
  const [useStream, setUseStream] = useState(true);

  // vídeo (snapshot double-buffer)
  const [currentFrameUri, setCurrentFrameUri] = useState("");
  const [nextFrameUri, setNextFrameUri] = useState("");
  const loadingNextRef = useRef(false);

  // joystick (UI)
  const [joy, setJoy] = useState({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs do ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  /* ===== ESP STATUS ===== */
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(`OK • ip:${j.ip} • mode:${j.mode} • led:${j.led ? "on" : "off"} • pump:${j.pump ? "on" : "off"}`);
    } catch {
      setStatusText(T.statusFail);
    } finally {
      setIsChecking(false);
    }
  }
  async function toggleLed() { try { const t = !ledOn; await fetch(ledUrl(t)); setLedOn(t); } catch { setStatusText(T.statusFail); } }
  async function togglePump() { try { const t = !pumpOn; await fetch(pumpUrl(t)); setPumpOn(t); } catch { setStatusText(T.statusFail); } }

  useEffect(() => { pingStatus(); const id = setInterval(pingStatus, 5000); return () => clearInterval(id); }, [statusUrl]);

  /* ===== Vídeo via snapshots — ~25fps (fallback) ===== */
  useEffect(() => {
    if (useStream) return;
    let stop = false;
    const FPS_INTERVAL = 40;
    setCurrentFrameUri(`${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`);
    const tick = () => {
      if (stop) return;
      if (!loadingNextRef.current) {
        const url = `${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`;
        setNextFrameUri(url);
      }
      setTimeout(tick, FPS_INTERVAL);
    };
    tick();
    return () => { stop = true; };
  }, [server, useStream]);

  function onNextLoadStart() { loadingNextRef.current = true; }
  function onNextShown() {
    if (nextFrameUri) setCurrentFrameUri(nextFrameUri);
    loadingNextRef.current = false;
  }

  /* ===== DETECT LOOP ===== */
  useEffect(() => {
    if (!detectOn) { setPeopleCount(0); setAnimalCount(0); return; }
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server.replace(/\/+$/, "")}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && j.ok) {
          // fogo
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));

          // pessoas/animais vindos do servidor
          const objsRoot = (j.objects && j.objects.objects) ? j.objects.objects : [];
          const people = (objsRoot as ObjDet[]).filter(o => o.label === "person").length;
          const animals = (objsRoot as ObjDet[]).filter(o => o.label !== "person").length;

          setPeopleCount(people);
          setAnimalCount(animals);
        }
      } catch {
        if (!stop) {
          setIsFire(false); setFireScore(0);
          setPeopleCount(0); setAnimalCount(0);
        }
      } finally {
        if (!stop) setTimeout(loop, 120); // ~8–9 Hz de estado (leve)
      }
    };
    loop();
    return () => { stop = true; };
  }, [detectOn, server]);

  /* ===== Registro automático ===== */
  useEffect(() => {
    const rising = !wasFireRef.current && isFire;
    const now = Date.now();
    const cooldownOk = now - lastSaveRef.current >= SAVE_COOLDOWN_MS;
    if (detectOn && rising && cooldownOk && !saving) {
      (async () => {
        try {
          setSaving(true);
          await saveEventFromServer(server);
          lastSaveRef.current = Date.now();
          setStatusText(T.eventSavedAt(new Date(lastSaveRef.current).toLocaleString()));
        } catch {
          setStatusText(T.noVideo);
        } finally {
          setSaving(false);
        }
      })();
    }
    wasFireRef.current = isFire;
  }, [detectOn, isFire, server, saving, T]);

  /* ===== Joystick ===== */
  const RADIUS = 64, KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS, ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny); if (len > 1) { nx /= len; ny /= len; }
        setJoy({ x: nx, y: -ny });
      },
      onPanResponderRelease: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
      onPanResponderTerminate: () => { setDragging(false); setJoy({ x: 0, y: 0 }); },
    })
  ).current;

  const knobLeft = joy.x * RADIUS, knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* Linha: ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>{T.espIp}</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder={T.placeholderIp}
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType="numeric"
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : T.test}</Text>
        </Pressable>

        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? T.ledOn : T.ledOff}</Text>
        </Pressable>

        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? T.pumpOn : T.pumpOff}</Text>
        </Pressable>
      </View>

      {/* Linha: Servidor IA + Detectar + Modo de Visão */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>{T.server}</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder={T.placeholderServer}
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? (saving ? T.saving : T.detecting) : T.detect}</Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>{statusText}</Text>
      </View>

      {/* Seletor de visão */}
      <View style={[styles.statusRow, { gap: 6 }]}>
        <Text style={styles.label}>{T.view}</Text>
        <Pressable onPress={() => setUseStream(true)}  style={[styles.btn, useStream ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{T.stream}</Text>
        </Pressable>
        <Pressable onPress={() => setUseStream(false)} style={[styles.btn, !useStream ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{T.snapshot}</Text>
        </Pressable>
      </View>

      {/* Banners de estado */}
      {detectOn && (
        <>
          <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
            <Text style={styles.fireText}>{isFire ? T.fireOn(fireScore) : T.fireOff(fireScore)}</Text>
          </View>

          {/* NOVO: badges de pessoas / animais */}
          <View style={styles.badgesRow}>
            <View style={[styles.badge, { backgroundColor: "#1b2a4b" }]}>
              <Text style={styles.badgeTxt}>{T.people(peopleCount)}</Text>
            </View>
            <View style={[styles.badge, { backgroundColor: "#2b3a1b" }]}>
              <Text style={styles.badgeTxt}>{T.animals(animalCount)}</Text>
            </View>
          </View>
        </>
      )}

      {/* Vídeo (STREAM rápido OU SNAPSHOT fallback) */}
      <View style={{ flex: 1 }}>
        {useStream ? (
          <LiveMJPEG ip={ip} />
        ) : (
          <CrossfadeImage
            currentUri={currentFrameUri}
            nextUri={nextFrameUri}
            onNextLoadStart={onNextLoadStart}
            onNextShown={onNextShown}
          />
        )}
      </View>

      {/* Joystick overlay (UI) */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              {
                width: KNOB_R * 2, height: KNOB_R * 2, borderRadius: KNOB_R,
                transform: [{ translateX: knobLeft }, { translateY: knobTop }],
                opacity: dragging ? 1 : 0.9,
              },
            ]}
          />
          <Text style={styles.joyText}>x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}</Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },

  topbar: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 10,
    gap: 8,
    backgroundColor: PALETTE.card,
  },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: {
    backgroundColor: PALETTE.bg,
    color: "white",
    borderRadius: 8,
    paddingHorizontal: 10,
    paddingVertical: 8,
    borderWidth: 1,
    borderColor: "#1e2a4d",
    minWidth: 110,
  },
  btn: {
    backgroundColor: "#2b48ff",
    paddingHorizontal: 10,
    paddingVertical: 10,
    borderRadius: 10,
  },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },

  statusRow: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 8,
    gap: 8,
    backgroundColor: "#0f1730",
  },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },

  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },

  // NOVO: badges de pessoas/animais
  badgesRow: {
    flexDirection: "row",
    gap: 8,
    paddingHorizontal: 12,
    paddingVertical: 6,
    backgroundColor: "#0b0f1e",
  },
  badge: {
    paddingHorizontal: 10,
    paddingVertical: 6,
    borderRadius: 8,
  },
  badgeTxt: { color: "white", fontWeight: "700" },

  // Joystick
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});




Checklist rápido




Servidor rodando (/debug_models com objects_ok: true).


App: “Servidor” = http://SEU_PC:8000.


Câmera → botão Detectar.


Veja os badges atualizando: 👤 Pessoas: N e 🐾 Animais: M.




Se /objects mostra itens, mas o app ainda não conta, me manda um exemplo de JSON retornado por /detect (primeiras linhas) para eu alinhar o path exato (j.objects.objects).























































(.venv) PS D:\loud2025\hydrobot-server> uvicorn server_heuristic:app --host 0.0.0.0 --port 8000 --reload   
INFO:     Will watch for changes in these directories: ['D:\\loud2025\\hydrobot-server']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [17572] using WatchFiles
Process SpawnProcess-1:
Traceback (most recent call last):
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\multiprocessing\process.py", line 313, in _bootstrap
    self.run()
    ~~~~~~~~^^
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\multiprocessing\process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
    ~~~~~~^^^^^^^^^^^^^^^^^
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\server.py", line 65, in run
    return asyncio.run(self.serve(sockets=sockets))
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py", line 195, in run 
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py", line 118, in run 
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py", line 725, in 
run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\server.py", line 69, in serve
    await self._serve(sockets)
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\server.py", line 76, in _serve
    config.load()
    ~~~~~~~~~~~^^
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\config.py", line 434, in load
    self.loaded_app = import_from_string(self.app)
                      ~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\loud2025\hydrobot-server\.venv\Lib\site-packages\uvicorn\importer.py", line 19, in import_from_string
    module = importlib.import_module(module_str)
  File "C:\Users\yagohmz\AppData\Local\Programs\Python\Python313\Lib\importlib\__init__.py", line 88, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 1022, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1160, in get_code
  File "<frozen importlib._bootstrap_external>", line 1090, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "D:\loud2025\hydrobot-server\server_heuristic.py", line 259
    self._thread = threading.Thread(target=self._run, daemon=True)
IndentationError: unexpected indent
