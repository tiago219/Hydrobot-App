



CameraScreen.tsx

import { useEffect, useMemo, useRef, useState } from "react";
import {
  View,
  Text,
  TextInput,
  Pressable,
  StyleSheet,
  PanResponder,
  GestureResponderEvent,
  PanResponderGestureState,
  Image,
} from "react-native";

const PALETTE = { bg: "#000000", card: "#0d0d0d", border: "#1a1a1a", red: "#E53B2F", white: "#ffffff" };

// ajuste estes padr√µes se quiser
const DEFAULT_IP = "192.168.4.1";            // ESP32-CAM (AP)
const DEFAULT_SERVER = "http://192.168.4.2:8000"; // Servidor IA (PC no mesmo AP)

export default function CameraScreen() {
  // conex√µes
  const [ip, setIp] = useState(DEFAULT_IP);
  const [server, setServer] = useState(DEFAULT_SERVER);

  // estados ESP
  const [isChecking, setIsChecking] = useState(false);
  const [statusText, setStatusText] = useState<string>("‚Äî");
  const [ledOn, setLedOn] = useState<boolean>(false);
  const [pumpOn, setPumpOn] = useState<boolean>(false);

  // detec√ß√£o
  const [detectOn, setDetectOn] = useState<boolean>(false);
  const [isFire, setIsFire] = useState<boolean>(false);
  const [fireScore, setFireScore] = useState<number>(0);

  // ‚Äúv√≠deo‚Äù por snapshots do servidor
  const [frameUri, setFrameUri] = useState<string>("");

  // joystick (UI)
  const [joy, setJoy] = useState<{ x: number; y: number }>({ x: 0, y: 0 });
  const [dragging, setDragging] = useState(false);

  // URLs do ESP
  const statusUrl = useMemo(() => `http://${ip}/status`, [ip]);
  const ledUrl = useMemo(() => (on: boolean) => `http://${ip}/led?on=${on ? "1" : "0"}`, [ip]);
  const pumpUrl = useMemo(() => (on: boolean) => `http://${ip}/pump?on=${on ? "1" : "0"}`, [ip]);

  // consulta /status do ESP
  async function pingStatus() {
    try {
      setIsChecking(true);
      const r = await fetch(statusUrl, { method: "GET" });
      const j = await r.json();
      setLedOn(!!j.led);
      setPumpOn(!!j.pump);
      setStatusText(
        `OK ‚Ä¢ ip:${j.ip} ‚Ä¢ mode:${j.mode} ‚Ä¢ led:${j.led ? "on" : "off"} ‚Ä¢ pump:${j.pump ? "on" : "off"}`
      );
    } catch {
      setStatusText("Falha ao conectar. Confira o Wi-Fi HYDROBOT-CAM e o IP.");
    } finally {
      setIsChecking(false);
    }
  }

  async function toggleLed() {
    try {
      const t = !ledOn;
      await fetch(ledUrl(t));
      setLedOn(t);
    } catch {
      setStatusText("Erro ao alternar LED.");
    }
  }

  async function togglePump() {
    try {
      const t = !pumpOn;
      await fetch(pumpUrl(t));
      setPumpOn(t);
    } catch {
      setStatusText("Erro ao alternar bomba.");
    }
  }

  // ping peri√≥dico de status do ESP
  useEffect(() => {
    pingStatus();
    const id = setInterval(pingStatus, 5000);
    return () => clearInterval(id);
  }, [statusUrl]);

  // ===== V√≠deo por snapshots do servidor =====
  useEffect(() => {
    let stop = false;
    const tick = () => {
      if (stop) return;
      // cache-buster (?ts=) para evitar cache da Image
      setFrameUri(`${server}/snapshot?ts=${Date.now()}`);
      setTimeout(tick, 250); // ~4 fps; ajuste conforme sua m√°quina/rede
    };
    tick();
    return () => {
      stop = true;
    };
  }, [server]);

  // polling de /detect do servidor de IA
  useEffect(() => {
    if (!detectOn) return;
    let stop = false;
    const loop = async () => {
      try {
        const r = await fetch(`${server}/detect`, { method: "GET" });
        const j = await r.json();
        if (!stop && j && j.ok) {
          setIsFire(!!j.isFire);
          setFireScore(Number(j.score || 0));
        }
      } catch {
        if (!stop) {
          setIsFire(false);
          setFireScore(0);
        }
      } finally {
        if (!stop) setTimeout(loop, 500); // ~2 Hz
      }
    };
    loop();
    return () => {
      stop = true;
    };
  }, [detectOn, server]);

  // joystick (UI)
  const RADIUS = 64;
  const KNOB_R = 22;
  const pan = useRef(
    PanResponder.create({
      onStartShouldSetPanResponder: () => true,
      onMoveShouldSetPanResponder: () => true,
      onPanResponderGrant: () => setDragging(true),
      onPanResponderMove: (_e: GestureResponderEvent, g: PanResponderGestureState) => {
        let nx = g.dx / RADIUS;
        let ny = g.dy / RADIUS;
        const len = Math.hypot(nx, ny);
        if (len > 1) {
          nx /= len;
          ny /= len;
        }
        setJoy({ x: nx, y: -ny }); // y positivo para cima
      },
      onPanResponderRelease: () => {
        setDragging(false);
        setJoy({ x: 0, y: 0 });
      },
      onPanResponderTerminate: () => {
        setDragging(false);
        setJoy({ x: 0, y: 0 });
      },
    })
  ).current;

  const knobLeft = joy.x * RADIUS;
  const knobTop = -joy.y * RADIUS;

  return (
    <View style={styles.container}>
      {/* Linha: ESP + LED + BOMBA */}
      <View style={styles.topbar}>
        <Text style={styles.label}>ESP IP:</Text>
        <TextInput
          value={ip}
          onChangeText={setIp}
          placeholder="192.168.4.1"
          autoCapitalize="none"
          autoCorrect={false}
          style={styles.input}
          keyboardType="numeric"
        />
        <Pressable onPress={pingStatus} style={styles.btn}>
          <Text style={styles.btnText}>{isChecking ? "..." : "Testar"}</Text>
        </Pressable>

        <Pressable onPress={toggleLed} style={[styles.btn, ledOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{ledOn ? "LED ON" : "LED OFF"}</Text>
        </Pressable>

        <Pressable onPress={togglePump} style={[styles.btn, pumpOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{pumpOn ? "BOMBA ON" : "BOMBA OFF"}</Text>
        </Pressable>
      </View>

      {/* Linha: Servidor IA + Detectar */}
      <View style={styles.statusRow}>
        <Text style={styles.label}>Servidor:</Text>
        <TextInput
          value={server}
          onChangeText={setServer}
          placeholder="http://192.168.4.2:8000"
          autoCapitalize="none"
          autoCorrect={false}
          style={[styles.input, { flex: 1 }]}
        />
        <Pressable onPress={() => setDetectOn((v) => !v)} style={[styles.btn, detectOn ? styles.btnOn : styles.btnOff]}>
          <Text style={styles.btnText}>{detectOn ? "Detectando" : "Detectar"}</Text>
        </Pressable>
      </View>

      {/* Status curto */}
      <View style={[styles.statusRow, { paddingTop: 4, paddingBottom: 8 }]}>
        <Text numberOfLines={2} style={styles.status}>
          {statusText}
        </Text>
      </View>

      {/* Banner de fogo */}
      {detectOn && (
        <View style={[styles.fireBanner, isFire ? styles.fireOn : styles.fireOff]}>
          <Text style={styles.fireText}>
            {isFire ? `üî• FOGO ‚Ä¢ score ${fireScore.toFixed(2)}` : `Sem fogo ‚Ä¢ score ${fireScore.toFixed(2)}`}
          </Text>
        </View>
      )}

      {/* ‚ÄúV√≠deo‚Äù via snapshots do servidor */}
      <Image
        source={{ uri: frameUri }}
        style={styles.video}
        resizeMode="contain"
        onError={() => {
          // feedback visual simples; mant√©m o polling at√© voltar
          setStatusText("Sem v√≠deo (snapshot). Verifique o servidor /snapshot.");
        }}
      />

      {/* Joystick overlay (UI) */}
      <View pointerEvents="box-none" style={StyleSheet.absoluteFill}>
        <View style={[styles.joyWrap, { width: RADIUS * 2 + 24, height: RADIUS * 2 + 24 }]} {...pan.panHandlers}>
          <View style={[styles.joyBase, { width: RADIUS * 2, height: RADIUS * 2, borderRadius: RADIUS }]} />
          <View
            style={[
              styles.joyKnob,
              {
                width: KNOB_R * 2,
                height: KNOB_R * 2,
                borderRadius: KNOB_R,
                transform: [{ translateX: knobLeft }, { translateY: knobTop }],
                opacity: dragging ? 1 : 0.9,
              },
            ]}
          />
          <Text style={styles.joyText}>
            x: {joy.x.toFixed(2)}   y: {joy.y.toFixed(2)}
          </Text>
        </View>
      </View>
    </View>
  );
}

const styles = StyleSheet.create({
  container: { flex: 1, backgroundColor: PALETTE.bg },

  topbar: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 10,
    gap: 8,
    backgroundColor: PALETTE.card,
  },
  label: { color: "#cfd3d8", fontSize: 12 },
  input: {
    backgroundColor: PALETTE.bg,
    color: "white",
    borderRadius: 8,
    paddingHorizontal: 10,
    paddingVertical: 8,
    borderWidth: 1,
    borderColor: "#1e2a4d",
    minWidth: 110,
  },
  btn: {
    backgroundColor: "#2b48ff",
    paddingHorizontal: 10,
    paddingVertical: 10,
    borderRadius: 10,
  },
  btnOn: { backgroundColor: "#17a34a" },
  btnOff: { backgroundColor: "#374151" },
  btnText: { color: "white", fontWeight: "600" },

  statusRow: {
    flexDirection: "row",
    alignItems: "center",
    paddingHorizontal: 12,
    paddingVertical: 8,
    gap: 8,
    backgroundColor: "#0f1730",
  },
  status: { color: "#cbd5ff", flex: 1, fontSize: 12 },

  fireBanner: { alignItems: "center", justifyContent: "center", paddingVertical: 6 },
  fireOn: { backgroundColor: "#8b0000" },
  fireOff: { backgroundColor: "#223" },
  fireText: { color: "white", fontWeight: "800" },

  // v√≠deo por snapshots
  video: { flex: 1, backgroundColor: "black", width: "100%" },

  // Joystick
  joyWrap: { position: "absolute", bottom: 24, left: 24, alignItems: "center", justifyContent: "center" },
  joyBase: { position: "absolute", backgroundColor: "rgba(255,255,255,0.08)", borderWidth: 2, borderColor: "rgba(255,255,255,0.14)" },
  joyKnob: { position: "absolute", backgroundColor: "rgba(43,72,255,0.9)", borderWidth: 2, borderColor: "rgba(255,255,255,0.85)" },
  joyText: { position: "absolute", bottom: -22, color: "#cbd5ff", fontSize: 12 },
});









server_heuristic.py

# server_heuristic.py
# FastAPI + OpenCV ‚Äî N√ÉO mant√©m o /stream do ESP aberto.
# Em cada chamada a /snapshot ou /detect, abre o stream, l√™ 1 frame e fecha imediatamente.
import cv2, time, numpy as np
from typing import Optional, List, Tuple
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60
OPEN_TIMEOUT_S = 3.0          # tempo m√°ximo tentando abrir stream
READ_TIMEOUT_S = 1.0          # tempo m√°ximo tentando ler um frame ap√≥s abrir
JPEG_QUALITY = 85

app = FastAPI(title="HydroBot Fire (Heuristic, on-demand)", version="0.2.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

def open_and_read_one(ip: str) -> Optional[np.ndarray]:
    """
    Abre o stream MJPEG do ESP, tenta ler 1 frame e fecha.
    Retorna BGR np.ndarray ou None se falhar.
    """
    url = STREAM_URL_FMT.format(ip)
    t0 = time.time()
    cap = None
    try:
        # Tenta abrir por at√© OPEN_TIMEOUT_S
        while time.time() - t0 < OPEN_TIMEOUT_S:
            cap = cv2.VideoCapture(url)
            # alguns backends ignoram CAP_PROP_BUFFERSIZE, mas n√£o atrapalha
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            if cap.isOpened():
                break
            cap.release()
            cap = None
            time.sleep(0.15)
        if cap is None or not cap.isOpened():
            return None

        # Tenta ler 1 frame por at√© READ_TIMEOUT_S
        t1 = time.time()
        while time.time() - t1 < READ_TIMEOUT_S:
            ok, frame = cap.read()
            if ok and frame is not None:
                return frame
            time.sleep(0.01)
        return None
    finally:
        if cap is not None:
            cap.release()

def fire_score(frame: np.ndarray) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    # janela laranja/amarela t√≠pica de chama; ajuste se necess√°rio
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame: np.ndarray) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
            continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

@app.get("/status")
def status():
    # N√ÉO abre stream aqui para n√£o competir com o app.
    return {
        "ok": True,
        "camera_ip": CAMERA_IP,
        "model": "heuristic_on_demand",
        "note": "This server opens the ESP stream only per-request (snapshot/detect) and releases immediately.",
        "ts": int(time.time() * 1000),
    }

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    f = open_and_read_one(CAMERA_IP)
    if f is None:
        return {"ok": False, "error": "no frame (ESP busy or unreachable)"}
    ok, buf = cv2.imencode(".jpg", f, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])
    if not ok:
        return {"ok": False, "error": "encode failed"}
    return Response(content=buf.tobytes(), media_type="image/jpeg")

@app.get("/detect")
def detect():
    f = open_and_read_one(CAMERA_IP)
    if f is None:
        return {"ok": False, "error": "no frame (ESP busy or unreachable)"}
    s = fire_score(f)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "boxes": (fire_boxes(f) if is_fire else []),
        "threshold": SCORE_THRESHOLD,
        "ts": int(time.time() * 1000),
    }













































Isso normalmente acontece porque o OpenCV n√£o l√™ bem o MJPEG do ESP32-CAM (porta 81//stream) via VideoCapture. Resultado: o seu /snapshot est√° falhando (volta JSON de erro) e o <Image> do app nunca recebe um JPEG v√°lido.


A solu√ß√£o √© n√£o usar cv2.VideoCapture para esse stream e, em vez disso, ler manualmente o MJPEG e extrair o primeiro frame. Abaixo vai um server_heuristic.py substituto que:




usa requests para conectar no :81/stream,


encontra o boundary --frame, l√™ os cabe√ßalhos (Content-Length) e pega 1 JPEG,


devolve o JPEG em /snapshot,


usa o mesmo frame para /detect.






Instale a depend√™ncia:


pip install requests






Substitua seu server_heuristic.py por este


# server_heuristic.py
# FastAPI + leitura MANUAL do MJPEG do ESP32-CAM (sem VideoCapture).
# Em cada /snapshot ou /detect, abre o stream, extrai o primeiro frame JPEG e fecha.

import time
from typing import Optional, List
import numpy as np
import cv2
import requests
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ------- CONFIG -------
CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60
CONNECT_TIMEOUT = 3.0   # seg p/ conectar
READ_TIMEOUT = 3.0      # seg p/ ler dados do stream
JPEG_QUALITY = 85
# O ESP32 do seu c√≥digo usa boundary "frame"
BOUNDARY = b"--frame"

# ------- FASTAPI -------
app = FastAPI(title="HydroBot Fire (MJPEG manual)", version="0.3.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

# ------- MJPEG READER -------
def read_one_jpeg_from_mjpeg(ip: str) -> Optional[bytes]:
    """
    Abre http://<ip>:81/stream e extrai o PRIMEIRO JPEG usando o boundary '--frame'.
    Retorna bytes do JPEG ou None se falhar.
    """
    url = STREAM_URL_FMT.format(ip)
    t0 = time.time()

    try:
        with requests.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)) as r:
            if r.status_code != 200:
                return None

            # Vamos iterar o 'iter_content' com chunks pequenos e montar um buffer
            buf = b""
            # limites razo√°veis para n√£o ficar preso:
            MAX_BYTES = 2_000_000  # 2MB
            while time.time() - t0 < READ_TIMEOUT:
                for chunk in r.iter_content(chunk_size=4096):
                    if chunk:
                        buf += chunk
                        if len(buf) > MAX_BYTES:
                            # stream estranho/ruim
                            return None

                        # Procurar boundary completo "\r\n--frame" (√†s vezes come√ßa direto com --frame)
                        # Para robustez, procure simplesmente por BOUNDARY
                        i = buf.find(BOUNDARY)
                        if i == -1:
                            continue

                        # Tem um boundary, esperam-se headers at√© uma linha em branco \r\n\r\n
                        # Ex: Content-Type e Content-Length
                        # Procure o final dos headers a partir de i
                        hdr_start = i + len(BOUNDARY)
                        # Alguns servidores usam \r\n ap√≥s o boundary
                        # ent√£o pule \r\n opcionais
                        while hdr_start < len(buf) and buf[hdr_start:hdr_start+2] in (b"\r\n",):
                            hdr_start += 2

                        headers_end = buf.find(b"\r\n\r\n", hdr_start)
                        if headers_end == -1:
                            # headers incompletos; continue juntando
                            continue

                        headers_bytes = buf[hdr_start:headers_end]
                        headers_text = headers_bytes.decode("latin1", errors="ignore")
                        content_length = None
                        for line in headers_text.split("\r\n"):
                            if line.lower().startswith("content-length:"):
                                try:
                                    content_length = int(line.split(":", 1)[1].strip())
                                except:
                                    pass
                                break

                        img_start = headers_end + 4
                        if content_length is not None:
                            # Temos o tamanho, garanta que o buffer j√° tem tudo
                            if len(buf) < img_start + content_length:
                                # ainda n√£o chegou tudo
                                continue
                            jpeg_bytes = buf[img_start:img_start+content_length]
                            if jpeg_bytes.startswith(b"\xff\xd8") and jpeg_bytes.endswith(b"\xff\xd9"):
                                return jpeg_bytes
                            else:
                                # fallback: tenta decodificar mesmo assim
                                return jpeg_bytes
                        else:
                            # Sem Content-Length ‚Äì tenta achar o pr√≥ximo boundary para fechar o frame
                            j = buf.find(BOUNDARY, img_start)
                            if j != -1:
                                jpeg_bytes = buf[img_start:j]
                                # pode conter \r\n no fim; remova lixo do fim
                                # tente cortar at√© √∫ltimo marcador de fim JPEG
                                end = jpeg_bytes.rfind(b"\xff\xd9")
                                if end != -1:
                                    jpeg_bytes = jpeg_bytes[:end+2]
                                return jpeg_bytes
                            # ainda n√£o chegou o pr√≥ximo boundary, continue lendo
                if time.time() - t0 >= READ_TIMEOUT:
                    break
    except requests.exceptions.RequestException:
        return None

    return None

def decode_jpeg_to_bgr(jpeg_bytes: bytes) -> Optional[np.ndarray]:
    arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)
    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    return img

# ------- FIRE HEURISTIC -------
def fire_score(frame: np.ndarray) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame: np.ndarray) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
          continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

# ------- ENDPOINTS -------
@app.get("/status")
def status():
    return {"ok": True, "camera_ip": CAMERA_IP, "model": "mjpeg_manual", "ts": int(time.time() * 1000)}

class ConfigIn(BaseModel):
    camera_ip: str

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        return {"ok": False, "error": "no frame (ESP busy/unreachable or parse error)"}
    return Response(content=jpeg, media_type="image/jpeg")

@app.get("/detect")
def detect():
    jpeg = read_one_jpeg_from_mjpeg(CAMERA_IP)
    if jpeg is None:
        return {"ok": False, "error": "no frame (ESP busy/unreachable or parse error)"}
    frame = decode_jpeg_to_bgr(jpeg)
    if frame is None:
        return {"ok": False, "error": "decode failed"}
    s = fire_score(frame)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "boxes": (fire_boxes(frame) if is_fire else []),
        "threshold": SCORE_THRESHOLD,
        "ts": int(time.time() * 1000),
    }




Ajustes no App (se ainda n√£o fez)


No CameraScreen.tsx voc√™ j√° est√° chamando:


setFrameUri(`${server.replace(/\/+$/, "")}/snapshot?ts=${Date.now()}`);



Perfeito. S√≥ confirme na UI que:




Servidor = http://IP_DO_SEU_PC:8000


(Opcional) envie a config pro servidor (uma vez) se seu ESP n√£o usa 192.168.4.1:

curl -X POST http://IP_DO_PC:8000/config -H "Content-Type: application/json" -d "{\"camera_ip\":\"192.168.4.1\"}"







Dicas de diagn√≥stico r√°pido




Abra no navegador do celular (conectado ao AP do ESP):

http://IP_DO_PC:8000/snapshot

Se baixar/abrir uma imagem, o app tamb√©m deveria mostrar.


Se n√£o abrir:



verifique firewall do PC (libere Python/uvicorn na rede privada),


confira se o celular e o PC est√£o na mesma sub-rede do AP do ESP,


confirme o IP correto do PC no campo ‚ÄúServidor‚Äù.








Com essa leitura manual do MJPEG, o /snapshot volta a fornecer JPEG v√°lido e as imagens aparecem no app, liberando a detec√ß√£o de fogo tamb√©m.























































Beleza ‚Äî o problema √© que o server mant√©m o /stream do ESP aberto o tempo todo, competindo com o app. Abaixo est√° o server_heuristic.py refeito para N√ÉO segurar o stream: ele abre, l√™ 1 frame e fecha em cada /snapshot ou /detect. Assim, o app continua exibindo a c√¢mera via WebView :81/stream como antes, e o servidor s√≥ ‚Äúbelisca‚Äù um frame quando necess√°rio.


Cole este arquivo inteiro:


# server_heuristic.py
# FastAPI + OpenCV ‚Äî N√ÉO mant√©m o /stream do ESP aberto.
# Em cada chamada a /snapshot ou /detect, abre o stream, l√™ 1 frame e fecha imediatamente.
import cv2, time, numpy as np
from typing import Optional, List, Tuple
from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

CAMERA_IP = "192.168.4.1"
STREAM_URL_FMT = "http://{}:81/stream"
SCORE_THRESHOLD = 0.60
OPEN_TIMEOUT_S = 3.0          # tempo m√°ximo tentando abrir stream
READ_TIMEOUT_S = 1.0          # tempo m√°ximo tentando ler um frame ap√≥s abrir
JPEG_QUALITY = 85

app = FastAPI(title="HydroBot Fire (Heuristic, on-demand)", version="0.2.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

class ConfigIn(BaseModel):
    camera_ip: str

def open_and_read_one(ip: str) -> Optional[np.ndarray]:
    """
    Abre o stream MJPEG do ESP, tenta ler 1 frame e fecha.
    Retorna BGR np.ndarray ou None se falhar.
    """
    url = STREAM_URL_FMT.format(ip)
    t0 = time.time()
    cap = None
    try:
        # Tenta abrir por at√© OPEN_TIMEOUT_S
        while time.time() - t0 < OPEN_TIMEOUT_S:
            cap = cv2.VideoCapture(url)
            # alguns backends ignoram CAP_PROP_BUFFERSIZE, mas n√£o atrapalha
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            if cap.isOpened():
                break
            cap.release()
            cap = None
            time.sleep(0.15)
        if cap is None or not cap.isOpened():
            return None

        # Tenta ler 1 frame por at√© READ_TIMEOUT_S
        t1 = time.time()
        while time.time() - t1 < READ_TIMEOUT_S:
            ok, frame = cap.read()
            if ok and frame is not None:
                return frame
            time.sleep(0.01)
        return None
    finally:
        if cap is not None:
            cap.release()

def fire_score(frame: np.ndarray) -> float:
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    # janela laranja/amarela t√≠pica de chama; ajuste se necess√°rio
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    ratio = float(np.count_nonzero(mask)) / float(mask.size)
    v_mean = float(np.mean(hsv[..., 2])) / 255.0
    return min(1.0, ratio * 4.0 + v_mean * 0.2)

def fire_boxes(frame: np.ndarray) -> List[List[int]]:
    h, w = frame.shape[:2]
    scl = min(640, w)
    rs = cv2.resize(frame, (scl, int(h * scl / w)))
    hsv = cv2.cvtColor(rs, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, (5, 80, 120), (40, 255, 255))
    k = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k, 1)
    mask = cv2.morphologyEx(mask, cv2.MORPH_DILATE, k, 1)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    rx = w / float(scl)
    ry = h / float(rs.shape[0])
    boxes: List[List[int]] = []
    for c in cnts:
        x, y, bw, bh = cv2.boundingRect(c)
        if bw * bh < 600:
            continue
        boxes.append([int(x * rx), int(y * ry), int(bw * rx), int(bh * ry)])
    return boxes

@app.get("/status")
def status():
    # N√ÉO abre stream aqui para n√£o competir com o app.
    return {
        "ok": True,
        "camera_ip": CAMERA_IP,
        "model": "heuristic_on_demand",
        "note": "This server opens the ESP stream only per-request (snapshot/detect) and releases immediately.",
        "ts": int(time.time() * 1000),
    }

@app.post("/config")
def set_config(cfg: ConfigIn):
    global CAMERA_IP
    CAMERA_IP = cfg.camera_ip
    return {"ok": True, "camera_ip": CAMERA_IP}

@app.get("/snapshot")
def snapshot():
    f = open_and_read_one(CAMERA_IP)
    if f is None:
        return {"ok": False, "error": "no frame (ESP busy or unreachable)"}
    ok, buf = cv2.imencode(".jpg", f, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])
    if not ok:
        return {"ok": False, "error": "encode failed"}
    return Response(content=buf.tobytes(), media_type="image/jpeg")

@app.get("/detect")
def detect():
    f = open_and_read_one(CAMERA_IP)
    if f is None:
        return {"ok": False, "error": "no frame (ESP busy or unreachable)"}
    s = fire_score(f)
    is_fire = bool(s >= SCORE_THRESHOLD)
    return {
        "ok": True,
        "isFire": is_fire,
        "score": round(float(s), 3),
        "boxes": (fire_boxes(f) if is_fire else []),
        "threshold": SCORE_THRESHOLD,
        "ts": int(time.time() * 1000),
    }



O que mudou (e por qu√™)




Sem thread e sem VideoCapture persistente: nada mant√©m o /stream ocupado.


Cada /snapshot e /detect faz open ‚Üí read 1 frame ‚Üí release.

Isso permite que o app fique conectado ao /stream de forma cont√≠nua, ‚Äúcomo estava antes‚Äù.


/status n√£o tenta abrir nada (s√≥ informa config).






Dica: se ainda rolar disputa rara (alguns firmwares aceitam apenas 1 conex√£o no exato instante), aumente OPEN_TIMEOUT_S para 5.0‚Äì7.0 e READ_TIMEOUT_S para 2.0.




No app voc√™ pode voltar para a vers√£o anterior (WebView fixo no :81/stream, sem altern√¢ncia). N√£o √© obrigat√≥rio mudar nada no app se ele j√° estava nessa forma ‚Äúantiga‚Äù.


